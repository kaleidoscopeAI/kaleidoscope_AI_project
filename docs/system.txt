Okay, here is the updated file structure incorporating the new components (LLMClient, CodeAnalyzer, FunctionRegistry) and the updated VisualNode and TaskManagerApp logic you provided:

/unravel_ai_project/
|-- src/
|   |-- core/
|   |   |-- __init__.py
|   |   |-- laws.py           # CoreLaws definition (placeholder from system.txt)
|   |   |-- base_node.py      # CapabilityNode base class (placeholder from system.txt)
|   |-- nodes/
|   |   |-- __init__.py
|   |   |-- text_node.py      # TextNode implementation (from system.txt)
|   |   |-- visual_node.py    # VisualNode implementation (NEW - provided by you)
|   |   |-- simulation_node.py # DataSimulationNode example (from system.txt)
|   |-- llm/
|   |   |-- __init__.py
|   |   |-- client.py         # LLMClient implementation (NEW - provided by you)
|   |   |-- processor.py      # LLMProcessor class (from system.txt)
|   |-- error/
|   |   |-- __init__.py
|   |   |-- definitions.py    # Error enums, dataclasses (from system.txt)
|   |   |-- handler.py        # ErrorManager, Registry, etc. (from system.txt)
|   |-- scheduler/
|   |   |-- __init__.py
|   |   |-- definitions.py    # Task enums, dataclasses (from system.txt)
|   |   |-- resource_monitor.py # ResourceMonitor class (from system.txt)
|   |   |-- manager.py        # OptimizedTaskScheduler class (from system.txt)
|   |-- reconstruction/
|   |   |-- __init__.py
|   |   |-- config.py         # ReconstructionConfig dataclass (from system.txt)
|   |   |-- engine.py         # ReconstructionEngine class (from system.txt, requires LLMClient)
|   |-- utils/
|   |   |-- __init__.py
|   |   |-- config_loader.py  # Simple config loading helper (from system.txt)
|   |   |-- function_resolver.py # FunctionRegistry implementation (NEW - provided by you)
|   |-- main/
|   |   |-- __init__.py
|   |   |-- task_analyzer.py  # CodeAnalyzer implementation (NEW - provided by you)
|   |   |-- task_manager_app.py # TaskManagerApp logic (UPDATED - with new tasks/components)
|   |   |-- cli.py            # Main entry point (from system.txt, uses TaskManagerApp)
|-- llm_config.json           # Example configuration for LLMProcessor
|-- app_config.json           # Optional: Main application configuration
|-- requirements.txt          # Python dependencies (Needs updating with new libs)
|-- README.md                 # Recommended
|-- models/                   # Optional: Directory for downloaded models
|   |-- vision/               # Directory for vision models used by VisualNode
|   |   |-- ssd_mobilenet_v2_coco_quant.tflite # Example model
|   |   |-- shape_predictor_68_face_landmarks.dat # Example model
|-- .llm_cache/               # Optional: Directory for LLMClient response caching
|-- unravel_ai_workdir/       # Default directory for outputs/persistence
This structure now reflects the addition of your LLMClient, VisualNode, FunctionRegistry, and CodeAnalyzer implementations, along with the necessary updates to TaskManagerApp.


Sources and related content
system

TXT





systemfixe...additions3
TXT

systemadditions
TXT

error_handler
TXT

error_handler (1)
TXT

error_handler (2)
TXT

error_handler (3)
TXT

laws
TXT

laws (1)
TXT

llm_client
TXT

logging_config
TXT

write the rest of the scxripts using system

TXT

systemfixesandadditions3

TXT

systemadditions

TXT scriprts. the rest of the scripts are the ones completed so far. use canvas




Show thinking
Okay, I will start creating the full-fledged "Kaleidoscope AI" system by integrating the components you've shared and generating the necessary code. I'll begin by completing the system_upgrade_module.py, adding the logging_config.py, and ensuring the latest versions of error_handler.py, laws.py, and llm_client.py are used, referencing the files you uploaded.

If the code exceeds the response limit, I will pause, and you can reply with 'k' to continue.

1. src/utils/logging_config.py (New file)
(Based on logging_config.txt)

Python

# src/utils/logging_config.py
import os
import sys
import logging
import logging.handlers
from pathlib import Path

def configure_logging(log_dir="unravel_ai_workdir/logs", log_level=logging.INFO):
    """
    Configure logging for the entire application with rotating file handlers
    and proper formatting.

    Args:
        log_dir (str): Directory where log files will be stored
        log_level (int): Logging level for the console handler

    Returns:
        logging.Logger: The configured root logger
    """
    # Create log directory if it doesn't exist
    log_dir_path = Path(log_dir)
    log_dir_path.mkdir(parents=True, exist_ok=True) # [cite: 3017]

    # Create formatters
    console_formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(name)s: %(message)s')
    file_formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(name)s (%(filename)s:%(lineno)d): %(message)s')

    # Create console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(log_level)
    console_handler.setFormatter(console_formatter)

    # Create file handlers
    main_log_file = log_dir_path / 'unravel_ai.log'
    error_log_file = log_dir_path / 'errors.log'

    main_file_handler = logging.handlers.RotatingFileHandler(
        main_log_file,
        maxBytes=10*1024*1024,  # 10 MB
        backupCount=5
    ) # [cite: 3018]
    main_file_handler.setLevel(log_level)
    main_file_handler.setFormatter(file_formatter)

    error_file_handler = logging.handlers.RotatingFileHandler(
        error_log_file,
        maxBytes=10*1024*1024,  # 10 MB
        backupCount=5
    ) # [cite: 2477]
    error_file_handler.setLevel(logging.ERROR)
    error_file_handler.setFormatter(file_formatter)

    # Configure root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.DEBUG)  # Set to lowest level, handlers will filter # [cite: 3019]

    # Remove existing handlers to avoid duplicates
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)

    # Add handlers to root logger
    root_logger.addHandler(console_handler)
    root_logger.addHandler(main_file_handler)
    root_logger.addHandler(error_file_handler)

    # Configure specific library loggers to be less verbose
    for logger_name, logger_level in [
        ('urllib3', logging.WARNING),
        ('PIL', logging.WARNING), # [cite: 3020]
        ('matplotlib', logging.WARNING),
        ('requests', logging.WARNING),
        ('transformers', logging.WARNING),
        ('asyncio', logging.WARNING),
        ('docker', logging.WARNING),
        ('aiohttp', logging.WARNING),
        ('sqlalchemy', logging.WARNING),
        ('pydantic', logging.WARNING),
        ('uvicorn', logging.WARNING)
    ]:
        logging.getLogger(logger_name).setLevel(logger_level) # [cite: 2479]

    logging.info("Logging configured successfully") # [cite: 3021]

    return root_logger

def get_logger(name, log_level=None):
    """
    Get a named logger with optional level override.

    Args:
        name (str): Name for the logger
        log_level (int, optional): Override default logging level

    Returns:
        logging.Logger: The configured logger
    """
    logger = logging.getLogger(name) # [cite: 3022]
    if log_level is not None:
        logger.setLevel(log_level)
    return logger

if __name__ == "__main__":
    # Test the logging configuration if run directly
    logger = configure_logging() # [cite: 3023]
    logger.debug("This is a debug message")
    logger.info("This is an info message")
    logger.warning("This is a warning message")
    logger.error("This is an error message")
    logger.critical("This is a critical message")

    # Test named logger
    test_logger = get_logger("test_module")
    test_logger.info("This is a message from the test module")
2. src/error/handler.py (Updated)
(Using the complete version from error_handler.txtand ensuring imports from logging_config)

Python

# src/error/handler.py
import os
import sys
import time
import traceback
import logging
import json
import hashlib
import asyncio
import random
import threading
from enum import Enum, auto
from dataclasses import dataclass, field, asdict
from typing import Dict, List, Any, Optional, Union, Tuple, Callable, Type, Set
from contextlib import contextmanager

# Use the new logging configuration
from src.utils.logging_config import get_logger

# Import definitions from the same directory
from .definitions import ErrorSeverity, ErrorCategory, ErrorContext, EnhancedError

logger = get_logger(__name__) # Use configured logger # [cite: 2482]

# --- ErrorHandlerRegistry ---
class ErrorHandlerRegistry:
    """Registry of error handlers for different categories and severities""" # [cite: 2483]
    def __init__(self):
        self.handlers: Dict[Tuple[Optional[ErrorCategory], Optional[ErrorSeverity]], List[Callable]] = {}
        self.default_handlers: List[Callable[[EnhancedError], None]] = []

    def register_handler(
        self,
        handler: Callable[[EnhancedError], None],
        category: Optional[ErrorCategory] = None,
        severity: Optional[ErrorSeverity] = None,
        is_default: bool = False
    ):
        """Register an error handler.""" # [cite: 2484]
        if is_default:
            if handler not in self.default_handlers:
                self.default_handlers.append(handler)
                logger.debug(f"Registered default error handler: {handler.__name__}")
            return

        key = (category, severity) # Can register for category only, severity only, or both # [cite: 2485]
        if key not in self.handlers:
            self.handlers[key] = []
        if handler not in self.handlers[key]:
             self.handlers[key].append(handler)
             logger.debug(f"Registered handler '{handler.__name__}' for Key={key}")

    def get_handlers(self, error: EnhancedError) -> List[Callable]:
        """Get all applicable handlers for an error, from specific to general.""" # [cite: 2486]
        applicable_handlers = []
        keys_to_check = [
            (error.category, error.severity),  # Most specific
            (error.category, None),            # Category specific
            (None, error.severity),            # Severity specific
        ]

        # Add handlers in order of specificity # [cite: 2487]
        for key in keys_to_check:
            if key in self.handlers:
                 # Add handlers only if not already added
                 for handler in self.handlers[key]:
                     if handler not in applicable_handlers: # [cite: 2488]
                          applicable_handlers.append(handler)

        # Add default handlers at the end
        for handler in self.default_handlers:
             if handler not in applicable_handlers:
                 applicable_handlers.append(handler)

        return applicable_handlers # [cite: 2489]

# --- ErrorManager (Singleton) ---
class ErrorManager:
    """Central error management system (Singleton)."""
    _instance = None
    _initialized = False # Class variable to track initialization

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super(ErrorManager, cls).__new__(cls)
        return cls._instance

    def __init__(self, error_log_path: str = None, logger_instance=None):
        """Initialize the error manager."""
        if ErrorManager._initialized: # Prevent re-initialization # [cite: 2490]
            return

        # Default error log path in workdir
        if error_log_path is None:
            # Use environment variable or default path
            workdir = os.environ.get("KALEIDOSCOPE_DATA_DIR", "data") # Use Kaleidoscope convention
            log_dir = os.path.join(workdir, "logs")
            os.makedirs(log_dir, exist_ok=True)
            error_log_path = os.path.join(log_dir, "errors.json")

        self.error_log_path = error_log_path # [cite: 2491]
        self.logger = logger_instance or logger # Use configured logger
        self.registry = ErrorHandlerRegistry()
        self.recent_errors: List[EnhancedError] = []
        self.max_recent_errors = 100
        self.error_counts: Dict[ErrorCategory, int] = {category: 0 for category in ErrorCategory}
        self.recovery_strategies: Dict[ErrorCategory, List[Callable]] = {}
        ErrorManager._initialized = True # Mark as initialized

        # Thread safety # [cite: 2492]
        self._lock = threading.RLock()

        # Load persistent errors? (Optional) # [cite: 2493]
        # self._load_persistent_errors()

        # Register built-in default handlers
        self._register_builtin_handlers()
        self.logger.info("ErrorManager initialized.")

    def _register_builtin_handlers(self):
        """Register built-in default error handlers."""
        self.registry.register_handler(self._log_error_handler, is_default=True)
        # Save critical/fatal errors by default
        self.registry.register_handler(self._save_error_handler, severity=ErrorSeverity.CRITICAL)
        self.registry.register_handler(self._save_error_handler, severity=ErrorSeverity.FATAL)

    def register_recovery_strategy(self, strategy: Callable[[EnhancedError], bool], category: ErrorCategory): # [cite: 2494]
        """Register a recovery strategy for an error category."""
        with self._lock:
            if category not in self.recovery_strategies:
                self.recovery_strategies[category] = []
            if strategy not in self.recovery_strategies[category]:
                 self.recovery_strategies[category].append(strategy)
                 self.logger.debug(f"Registered recovery strategy '{strategy.__name__}' for category {category.name}") # [cite: 2585]

    def handle_error(self, error: EnhancedError) -> bool:
        """Process an EnhancedError object."""
        with self._lock:
            # Add to recent errors queue
            self.recent_errors.append(error)
            if len(self.recent_errors) > self.max_recent_errors:
                self.recent_errors.pop(0) # [cite: 2586]

            # Update error counts
            self.error_counts[error.category] = self.error_counts.get(error.category, 0) + 1

        # Log the error using its own log method (which uses severity)
        error.log(self.logger)

        # Get and run handlers
        handlers = self.registry.get_handlers(error)
        handled_by_specific = False
        if not handlers: # [cite: 2587]
             self.logger.warning(f"No handlers registered for error: {error.error_id} ({error.category.name}, {error.severity.name})")

        for handler in handlers:
            try:
                handler(error)
                handled_by_specific = True # Consider handled if any handler runs
            except Exception as handler_e: # [cite: 2588]
                self.logger.error(f"Error within error handler '{handler.__name__}': {handler_e}", exc_info=True)

        # Try recovery strategies only if not handled adequately by specific handlers? (Design choice) # [cite: 2589]
        # Or always try recovery? Let's always try recovery if available.
        recovery_successful = False # [cite: 2590]
        with self._lock:
            if error.category in self.recovery_strategies:
                self.logger.info(f"Attempting recovery strategies for error {error.error_id} (Category: {error.category.name})")
                for strategy in self.recovery_strategies[error.category]:
                    try:
                        if strategy(error): # Strategy returns True if successful # [cite: 2591]
                            self.logger.info(f"Recovery strategy '{strategy.__name__}' succeeded for error {error.error_id}")
                            recovery_successful = True
                            break # Stop after first successful recovery # [cite: 2592]
                    except Exception as recovery_e:
                        self.logger.error(f"Error during recovery strategy '{strategy.__name__}': {recovery_e}", exc_info=True)

        return recovery_successful # Return if recovery was successful

    # --- Built-in Handlers ---
    def _log_error_handler(self, error: EnhancedError): # [cite: 2593]
        """Default handler to ensure error is logged (already done by error.log)."""
        pass # Logging is handled by error.log() called in handle_error

    def _save_error_handler(self, error: EnhancedError):
        """Default handler to save severe errors to a JSON log file."""
        self.logger.debug(f"Executing save error handler for {error.error_id}")
        errors = []
        try:
            if os.path.exists(self.error_log_path): # [cite: 2594]
                try:
                    # Ensure thread-safe read
                    with self._lock:
                        with open(self.error_log_path, 'r') as f:
                            content = f.read()
                    if content: # Avoid error on empty file # [cite: 2595]
                        errors = json.loads(content)
                    if not isinstance(errors, list): # Handle corrupted file # [cite: 2596]
                        self.logger.warning(f"Error log file '{self.error_log_path}' corrupted. Starting fresh.")
                        errors = []
                except (json.JSONDecodeError, IOError) as e:
                    self.logger.warning(f"Error reading error log file '{self.error_log_path}': {e}. Starting fresh.")
                    errors = [] # [cite: 2597]

            errors.append(error.to_dict())

            # Optional: Limit log file size (e.g., keep last N errors)
            max_log_entries = 500
            if len(errors) > max_log_entries:
                 errors = errors[-max_log_entries:]

            # Ensure directory exists
            os.makedirs(os.path.dirname(self.error_log_path), exist_ok=True) # [cite: 2598]

            # Ensure thread-safe write
            with self._lock:
                with open(self.error_log_path, 'w') as f:
                    json.dump(errors, f, indent=2)
            self.logger.debug(f"Saved error {error.error_id} to {self.error_log_path}")

        except Exception as e:
            # Log error about saving the error itself # [cite: 2599]
            self.logger.error(f"CRITICAL: Failed to save error {error.error_id} to log file '{self.error_log_path}': {e}", exc_info=True)

    # --- Error Creation and Handling Helpers ---
    def create_error(
        self,
        message: str,
        category: ErrorCategory = ErrorCategory.UNKNOWN,
        severity: ErrorSeverity = ErrorSeverity.ERROR,
        exception: Optional[Exception] = None,
        operation: Optional[str] = None, # [cite: 2600]
        component: Optional[str] = None,
        task_id: Optional[str] = None,
        **context_kwargs
    ) -> EnhancedError:
        """Creates an EnhancedError object."""
        context = ErrorContext(
            operation=operation or "N/A",
            component=component,
            task_id=task_id,
            **context_kwargs # [cite: 2601]
        )
        error = EnhancedError(
            message=message,
            category=category,
            severity=severity,
            exception=exception,
            context=context
        )
        return error # [cite: 2602]

    def handle_exception(
        self,
        exception: Exception,
        category: ErrorCategory = ErrorCategory.UNKNOWN,
        severity: ErrorSeverity = ErrorSeverity.ERROR,
        operation: Optional[str] = None,
        component: Optional[str] = None,
        task_id: Optional[str] = None,
        reraise: bool = False, # Option to re-raise after handling # [cite: 2603]
        **context_kwargs
    ) -> Optional[EnhancedError]:
        """Creates an EnhancedError from an exception and handles it."""
        error = self.create_error(
            message=str(exception) or f"Exception of type {type(exception).__name__} occurred.",
            category=category,
            severity=severity,
            exception=exception,
            operation=operation, # [cite: 2604]
            component=component,
            task_id=task_id,
            **context_kwargs
        )
        self.handle_error(error)

        if reraise:
            raise exception
        return error

    @contextmanager
    def error_context( # [cite: 2605]
        self,
        operation: str,
        category: ErrorCategory = ErrorCategory.UNKNOWN,
        severity: ErrorSeverity = ErrorSeverity.ERROR,
        component: Optional[str] = None,
        task_id: Optional[str] = None,
        reraise: bool = True, # Default to re-raising after handling
        **context_kwargs
    ):
        """Context manager for handling exceptions within a block.""" # [cite: 2606]
        try:
            yield
        except Exception as e:
            self.handle_exception(
                exception=e,
                category=category,
                severity=severity,
                operation=operation, # [cite: 2607]
                component=component,
                task_id=task_id,
                reraise=reraise, # Control re-raising
                **context_kwargs
            )

# --- RetryManager ---
# (Keep implementation from error_handler.txt)
class RetryManager:
    """Manages retrying operations with various strategies.""" # [cite: 2608]
    def __init__(self, logger_instance=None):
        self.logger = logger_instance or logger
        self.default_max_retries = 3
        self.default_initial_delay = 1.0
        self.default_max_delay = 60.0
        self.default_backoff_factor = 2.0

    async def retry_async(
        self,
        operation: Callable,
        *args,
        max_retries: int = None, # [cite: 2609]
        initial_delay: float = None,
        max_delay: float = None,
        backoff_factor: float = None,
        retry_exceptions: Tuple[Type[Exception], ...] = (Exception,),
        operation_name: str = None, # Optional name for logging
        **kwargs
    ):
        """Retries an async operation with exponential backoff."""
        max_retries = max_retries if max_retries is not None else self.default_max_retries # [cite: 2610]
        initial_delay = initial_delay if initial_delay is not None else self.default_initial_delay
        max_delay = max_delay if max_delay is not None else self.default_max_delay
        backoff_factor = backoff_factor if backoff_factor is not None else self.default_backoff_factor
        op_name = operation_name or getattr(operation, '__name__', 'Unnamed Operation')

        last_exception = None
        for attempt in range(max_retries + 1):
            try: # [cite: 2611]
                return await operation(*args, **kwargs)
            except retry_exceptions as e:
                last_exception = e
                if attempt >= max_retries:
                    self.logger.error(f"All {max_retries+1} attempts failed for async operation '{op_name}'. Last error: {e}", exc_info=True) # [cite: 2612]
                    raise # Re-raise the last exception
                else:
                    delay = min(initial_delay * (backoff_factor ** attempt), max_delay)
                    # Add jitter to delay # [cite: 2613]
                    jitter = delay * 0.1
                    actual_delay = delay + random.uniform(-jitter, jitter)
                    self.logger.warning(f"Async operation '{op_name}' failed (Attempt {attempt+1}/{max_retries+1}). Retrying in {actual_delay:.2f}s. Error: {e}")
                    await asyncio.sleep(actual_delay) # [cite: 2614]
            except Exception as non_retry_e:
                 # If it's not an exception we should retry, re-raise immediately
                 self.logger.error(f"Non-retryable error during async operation '{op_name}': {non_retry_e}", exc_info=True)
                 raise non_retry_e

    def retry(
        self,
        operation: Callable, # [cite: 2705]
        *args,
        max_retries: int = None,
        initial_delay: float = None,
        max_delay: float = None,
        backoff_factor: float = None,
        retry_exceptions: Tuple[Type[Exception], ...] = (Exception,),
        operation_name: str = None,
        **kwargs
    ):
        """Retries a synchronous operation with exponential backoff.""" # [cite: 2706]
        # Similar logic as retry_async, but using time.sleep
        max_retries = max_retries if max_retries is not None else self.default_max_retries
        initial_delay = initial_delay if initial_delay is not None else self.default_initial_delay
        max_delay = max_delay if max_delay is not None else self.default_max_delay
        backoff_factor = backoff_factor if backoff_factor is not None else self.default_backoff_factor
        op_name = operation_name or getattr(operation, '__name__', 'Unnamed Operation') # [cite: 2707]

        last_exception = None
        for attempt in range(max_retries + 1):
            try:
                return operation(*args, **kwargs)
            except retry_exceptions as e:
                last_exception = e
                if attempt >= max_retries: # [cite: 2708]
                    self.logger.error(f"All {max_retries+1} attempts failed for operation '{op_name}'. Last error: {e}", exc_info=True) # [cite: 2709]
                    raise
                else:
                    delay = min(initial_delay * (backoff_factor ** attempt), max_delay)
                    jitter = delay * 0.1
                    actual_delay = delay + random.uniform(-jitter, jitter) # [cite: 2710]
                    self.logger.warning(f"Operation '{op_name}' failed (Attempt {attempt+1}/{max_retries+1}). Retrying in {actual_delay:.2f}s. Error: {e}")
                    time.sleep(actual_delay)
            except Exception as non_retry_e:
                 self.logger.error(f"Non-retryable error during operation '{op_name}': {non_retry_e}", exc_info=True) # [cite: 2711]
                 raise non_retry_e

# --- GracefulDegradation ---
# (Keep implementation from error_handler.txt)
class GracefulDegradation:
    """Implements fallback strategies for graceful degradation."""
    def __init__(self, error_manager: ErrorManager = None, logger_instance=None):
        self.error_manager = error_manager or ErrorManager()
        self.logger = logger_instance or logger
        self.fallback_strategies: Dict[str, Callable] = {} # Key: "component.function"
        self.degradation_state: Dict[str, bool] = {} # Track which components are degraded # [cite: 2712]

        # Register built-in strategies (example placeholders)
        # self._register_builtin_strategies() # Call this if you define built-ins

    def register_fallback(self, component: str, function: str, fallback: Callable):
        """Registers a fallback strategy for a component.function."""
        key = f"{component}.{function}"
        self.fallback_strategies[key] = fallback
        self.logger.debug(f"Registered fallback for {key}: {fallback.__name__}")

    def get_fallback(self, component: str, function: str) -> Optional[Callable]:
        """Gets the fallback strategy.""" # [cite: 2713]
        key = f"{component}.{function}"
        return self.fallback_strategies.get(key)

    @contextmanager
    def degradable_operation(self, component: str, function: str, *args, **kwargs):
        """Context manager for operations that can degrade gracefully."""
        key = f"{component}.{function}"
        try:
            # The 'yield' passes control back to the 'with' block # [cite: 2714]
            # If the block completes without error, we're done.
            yield # Result of the block (if any) is implicitly returned by 'with' statement if needed # [cite: 2715]
        except Exception as e:
            fallback = self.get_fallback(component, function)
            if fallback:
                self.logger.warning(f"Operation {key} failed with error: {e}. Attempting fallback {fallback.__name__}.", exc_info=True)
                self.degradation_state[key] = True # Mark as degraded # [cite: 2716]
                try:
                    # Execute the fallback function. # [cite: 2717]
                    # We need to return its result if the 'with' block expects one. # [cite: 2718]
                    # The context manager itself doesn't explicitly return,
                    # but the fallback execution happens here. # [cite: 2719]
                    fallback_result = fallback(original_exception=e, *args, **kwargs)
                    # If the fallback succeeds, we effectively swallow the original exception here. # [cite: 2720]
                    # The result of the 'with' block would be None unless the fallback returns something
                    # and the caller assigns the 'with' statement, which is unusual. # [cite: 2721]
                    # If the fallback *itself* raises an error, it propagates out. # [cite: 2722]
                    self.logger.info(f"Fallback for {key} completed.")
                    # How to return the fallback result? # [cite: 2723]
                    # The context manager protocol doesn't directly support this easily.
                    # Usually, the code inside the 'with' block would handle the result or expect None on fallback. # [cite: 2724]
                    # Or, the fallback could modify state that the code after 'with' checks. # [cite: 2725]
                    # For simplicity, we log and swallow the original exception if fallback runs without error. # [cite: 2726]
                    # If the fallback should *provide* a return value *instead* of the original block,
                    # the calling code needs modification, or the fallback needs to store the result somewhere accessible. # [cite: 2727]
                    # Let's assume the fallback logs or modifies state, and swallow the original error here if fallback succeeds. # [cite: 2728]
                    # If the fallback itself fails, THAT exception will propagate. # [cite: 2729]
                except Exception as fallback_e:
                     self.logger.error(f"Fallback function {fallback.__name__} for {key} also failed: {fallback_e}", exc_info=True)
                     raise e # Re-raise the *original* exception if fallback fails
            else:
                # No fallback available, handle the original error # [cite: 2730]
                self.logger.error(f"Operation {key} failed and no fallback registered. Error: {e}", exc_info=True)
                # Handle the exception using ErrorManager before re-raising
                self.error_manager.handle_exception(
                     exception=e,
                     category=ErrorCategory.UNKNOWN, # Or try to categorize better # [cite: 2731]
                     operation=f"{component}.{function}"
                )
                raise e # Re-raise the original exception

# --- ErrorMonitor ---
# (Keep implementation from error_handler.txt)
class ErrorMonitor:
    """Monitors errors, calculates trends, and triggers alerts."""
    def __init__(self, error_manager: ErrorManager = None, logger_instance=None):
        self.error_manager = error_manager or ErrorManager() # [cite: 2732]
        self.logger = logger_instance or logger
        self.alert_thresholds: Dict[ErrorCategory, Dict[str, int]] = {} # {category: {'count': N, 'window': Sec}}
        self.alert_callbacks: Dict[ErrorCategory, List[Callable]] = {}
        self.error_trends: Dict[ErrorCategory, List[float]] = {} # {category: [timestamp1, timestamp2,...]}
        self.last_alert_time: Dict[ErrorCategory, float] = {}
        self.alert_cooldown_seconds: int = 300 # Default 5 minutes

        # Automatically subscribe to errors handled by the ErrorManager instance? # [cite: 2733]
        # This requires ErrorManager to provide a way to subscribe, e.g., a callback list.
        # For now, assume add_error is called externally when an error occurs. # [cite: 2734]

    def set_alert_threshold(self, category: ErrorCategory, count_threshold: int, time_window_seconds: int = 3600): # [cite: 2735]
        """Sets an alert threshold for an error category."""
        if not isinstance(category, ErrorCategory):
             self.logger.error(f"Invalid category type for alert threshold: {type(category)}")
             return
        if count_threshold <= 0 or time_window_seconds <= 0:
             self.logger.error(f"Alert threshold count and window must be positive.") # [cite: 2736]
             return
        self.alert_thresholds[category] = {'count': count_threshold, 'window': time_window_seconds}
        self.logger.info(f"Alert threshold set for {category.name}: {count_threshold} errors in {time_window_seconds}s")

    def register_alert_callback(self, category: ErrorCategory, callback: Callable[[ErrorCategory, int], None]):
        """Registers a callback function to be triggered when an alert threshold is met."""
        if not isinstance(category, ErrorCategory):
             self.logger.error(f"Invalid category type for alert callback: {type(category)}") # [cite: 2737]
             return
        if category not in self.alert_callbacks:
            self.alert_callbacks[category] = []
        if callback not in self.alert_callbacks[category]:
            self.alert_callbacks[category].append(callback)
            self.logger.debug(f"Registered alert callback '{callback.__name__}' for category {category.name}")

    def add_error(self, error: EnhancedError):
        """Records an error and checks if alert thresholds are met.""" # [cite: 2738]
        if not isinstance(error, EnhancedError):
             self.logger.warning(f"Attempted to add non-EnhancedError to monitor: {type(error)}")
             return

        category = error.category
        timestamp = error.timestamp

        # Initialize trend list if needed
        if category not in self.error_trends:
            self.error_trends[category] = [] # [cite: 2739]

        # Add timestamp and prune old entries outside the largest window for this category
        # (Optimize by finding max window across all thresholds for this category if multiple exist)
        max_window = max(t['window'] for c, t in self.alert_thresholds.items() if c == category) if category in self.alert_thresholds else 3600 # Default window prune
        now = time.time()
        window_start = now - max_window # [cite: 2740]
        self.error_trends[category] = [t for t in self.error_trends[category] if t > window_start]
        self.error_trends[category].append(timestamp)

        # Check alert threshold for this specific category
        self._check_alert_threshold(category)

    def _check_alert_threshold(self, category: ErrorCategory):
        """Checks if the alert threshold for a specific category is reached."""
        if category not in self.alert_thresholds:
            return # [cite: 2741]

        threshold_info = self.alert_thresholds[category]
        count_threshold = threshold_info['count']
        time_window = threshold_info['window']
        now = time.time()

        # Count errors within the specific time window for this threshold
        window_start = now - time_window
        errors_in_window = [t for t in self.error_trends.get(category, []) if t > window_start]
        current_count = len(errors_in_window) # [cite: 2742]

        if current_count >= count_threshold:
            # Check cooldown
            last_alert = self.last_alert_time.get(category, 0)
            if now - last_alert > self.alert_cooldown_seconds:
                self._trigger_alert(category, current_count)
                self.last_alert_time[category] = now # Update last alert time # [cite: 2743]
            else:
                 self.logger.debug(f"Alert threshold for {category.name} reached ({current_count}), but within cooldown period.")
        else:
             # Reset last alert time if count drops below threshold? Optional. # [cite: 2744]
             pass

    def _trigger_alert(self, category: ErrorCategory, error_count: int):
        """Triggers registered alert callbacks for a category."""
        self.logger.warning(f"ALERT Triggered for {category.name}: {error_count} errors reached threshold.")
        if category in self.alert_callbacks:
            for callback in self.alert_callbacks[category]:
                try:
                    # Consider running callbacks in separate threads/tasks if they might block # [cite: 2745]
                    callback(category, error_count)
                except Exception as cb_e:
                    self.logger.error(f"Error executing alert callback '{callback.__name__}': {cb_e}", exc_info=True)

    def get_error_statistics(self) -> Dict[str, Any]:
        """Gets current error counts and trend statistics."""
        stats = { # [cite: 2746]
            'counts_by_category': {cat.name: count for cat, count in self.error_manager.error_counts.items()},
            'recent_error_count': len(self.error_manager.recent_errors),
            'trends': {} # Calculate trends on demand
        }
        now = time.time()
        for category, timestamps in self.error_trends.items():
            stats['trends'][category.name] = { # [cite: 2747]
                'last_minute': sum(1 for t in timestamps if t > now - 60),
                'last_10_minutes': sum(1 for t in timestamps if t > now - 600),
                'last_hour': sum(1 for t in timestamps if t > now - 3600),
                'total_tracked': len(timestamps) # [cite: 2748]
            }
        return stats

# --- Example Usage ---
if __name__ == "__main__":
    example_main() # [cite: 2751]
3. src/core/laws.py (Updated)
(Using the complete version from laws.txt/laws (1).txtand ensuring imports from logging_config)

Python

# src/core/laws.py
import random
import math
import uuid
import logging
import sys
import os

# Use the new logging configuration
from src.utils.logging_config import get_logger

logger = get_logger(__name__) # [cite: 2842]

class CoreLaws:
    """
    Defines the basic operational laws governing node behavior.
    These laws control energy dynamics, mutation, and learning rates
    for the capability nodes in the system.
    """
    def __init__(self, # [cite: 2874]
                 learning_rate: float = 0.01,
                 energy_decay: float = 0.1,
                 base_energy_gain: float = 1.0,
                 mutation_rate: float = 0.005,
                 energy_scaling: float = 1.0, # [cite: 2875]
                 adaptation_rate: float = 0.02,
                 cooperation_bonus: float = 0.15,
                 specialization_reward: float = 0.1,
                 law_id: str = None):
        """
        Initialize CoreLaws with operational parameters. # [cite: 2876]

        Args:
            learning_rate: Rate at which nodes improve with successful tasks
            energy_decay: Natural energy decay rate
            base_energy_gain: Base amount of energy gained from successful operations
            mutation_rate: Probability of mutation during reproduction
            energy_scaling: Scaling factor for energy operations
            adaptation_rate: Rate at which nodes adapt to recurring tasks # [cite: 2877]
            cooperation_bonus: Bonus for successful cooperative operations
            specialization_reward: Reward for specializing in particular capabilities
            law_id: Unique identifier for this law instance
        """
        self.learning_rate = learning_rate
        self.energy_decay = energy_decay
        self.base_energy_gain = base_energy_gain # [cite: 2878]
        self.mutation_rate = mutation_rate
        self.energy_scaling = energy_scaling
        self.adaptation_rate = adaptation_rate
        self.cooperation_bonus = cooperation_bonus
        self.specialization_reward = specialization_reward
        self.law_id = law_id or str(uuid.uuid4())

        # Track operational statistics
        self.stats = {
            "energy_granted": 0.0, # [cite: 2879]
            "energy_decayed": 0.0,
            "mutations": 0,
            "adaptation_events": 0,
            "cooperation_events": 0
        }

        logger.debug(f"CoreLaws initialized with ID: {self.law_id}")

    def apply_energy_dynamics(self, current_energy: float, task_success: bool = True, task_complexity: float = 1.0) -> float: # [cite: 2880]
        """
        Applies basic energy gain/decay with success-based adjustments. # [cite: 2881]
        Args:
            current_energy: Current energy level
            task_success: Whether the task was successful
            task_complexity: Complexity factor of the task (higher = more energy)

        Returns:
            float: Updated energy level
        """
        # Base decay applies to all nodes # [cite: 2882]
        decay = self.energy_decay * self.energy_scaling

        # Calculate energy gain based on success and complexity
        if task_success:
            # Successful tasks earn energy proportional to complexity
            gain = self.base_energy_gain * task_complexity * self.energy_scaling

            # Add randomness for variability (optional) # [cite: 2883]
            gain *= random.uniform(0.8, 1.2)
        else:
            # Failed tasks get minimal energy
            gain = self.base_energy_gain * 0.1 * self.energy_scaling

        # Update stats
        self.stats["energy_granted"] += gain # [cite: 2884]
        self.stats["energy_decayed"] += decay

        # Calculate new energy level, prevent negative energy
        new_energy = max(0.0, current_energy + gain - decay)

        return new_energy

    def calculate_learning_adjustment(self, capability_level: float, task_success: bool,
                                      consecutive_successes: int = 0) -> float: # [cite: 2885]
        """
        Calculates learning adjustment based on task results and history. # [cite: 2886]
        Args:
            capability_level: Current capability level (0.0 to 1.0)
            task_success: Whether the task was successful
            consecutive_successes: Number of consecutive successful tasks

        Returns:
            float: Adjustment to capability level
        """
        # Base adjustment depends on success/failure # [cite: 2887]
        if task_success:
            # Successful tasks improve capability
            # Learning gets harder as capability increases (diminishing returns)
            adjustment = self.learning_rate * (1.0 - capability_level**2)

            # Bonus for consecutive successes (momentum) # [cite: 2888]
            if consecutive_successes > 1:
                adjustment += self.learning_rate * 0.2 * min(consecutive_successes, 5)
        else:
            # Failed tasks slightly decrease capability
            adjustment = -self.learning_rate * 0.5

        # Ensure capability stays in valid range after adjustment # [cite: 2889]
        projected_capability = capability_level + adjustment
        if projected_capability > 1.0:
            adjustment = 1.0 - capability_level
        elif projected_capability < 0.0:
            adjustment = -capability_level

        return adjustment

    def should_mutate(self) -> bool: # [cite: 2890]
        """
        Determines if a mutation should occur based on mutation rate. # [cite: 2891]
        Returns:
            bool: True if mutation should occur
        """
        return random.random() < self.mutation_rate

    def apply_adaptation(self, capability_history: list) -> float:
        """
        Calculates adaptation bonus based on task history. # [cite: 2892]
        Args:
            capability_history: List of recent capability usages

        Returns:
            float: Adaptation bonus
        """
        if not capability_history:
            return 0.0

        # Count repeated tasks # [cite: 2893]
        task_counts = {}
        for task in capability_history:
            task_counts[task] = task_counts.get(task, 0) + 1

        # Find most frequent task
        most_frequent = max(task_counts.items(), key=lambda x: x[1])

        # Calculate adaptation bonus based on specialization # [cite: 2894]
        if most_frequent[1] >= 3:  # At least 3 repetitions
            self.stats["adaptation_events"] += 1
            return self.adaptation_rate * most_frequent[1] * self.specialization_reward

        return 0.0

    def calculate_cooperation_bonus(self, collaborating_nodes: int) -> float:
        """
        Calculates bonus for nodes working together. # [cite: 2895]

        Args:
            collaborating_nodes: Number of nodes collaborating

        Returns:
            float: Cooperation energy bonus
        """
        if collaborating_nodes <= 1:
            return 0.0

        # Bonus increases with more collaborators but has diminishing returns # [cite: 2896]
        bonus = self.cooperation_bonus * math.log(collaborating_nodes + 1, 2)
        self.stats["cooperation_events"] += 1

        return bonus

    def mutate(self):
        """
        Creates a potentially mutated version of the laws for offspring. # [cite: 2897]
        Returns:
            CoreLaws: A new, potentially mutated instance
        """
        # Only mutate if the condition is met
        if not self.should_mutate():
            return CoreLaws(
                learning_rate=self.learning_rate,
                energy_decay=self.energy_decay,
                base_energy_gain=self.base_energy_gain, # [cite: 2898]
                mutation_rate=self.mutation_rate,
                energy_scaling=self.energy_scaling,
                adaptation_rate=self.adaptation_rate,
                cooperation_bonus=self.cooperation_bonus,
                specialization_reward=self.specialization_reward
            ) # [cite: 2899]

        # Track mutation event
        self.stats["mutations"] += 1

        # Determine which parameters to mutate (randomly select 1-3)
        params_to_mutate = random.sample([
            "learning_rate", "energy_decay", "base_energy_gain",
            "mutation_rate", "energy_scaling", "adaptation_rate",
            "cooperation_bonus", "specialization_reward" # [cite: 2900]
        ], k=random.randint(1, 3))

        # Create new parameters dictionary starting with current values
        new_params = {
            "learning_rate": self.learning_rate,
            "energy_decay": self.energy_decay,
            "base_energy_gain": self.base_energy_gain,
            "mutation_rate": self.mutation_rate, # [cite: 2901]
            "energy_scaling": self.energy_scaling,
            "adaptation_rate": self.adaptation_rate,
            "cooperation_bonus": self.cooperation_bonus,
            "specialization_reward": self.specialization_reward
        }

        # Apply mutations to selected parameters
        for param in params_to_mutate: # [cite: 2902]
            # Different mutation strategies for different parameters
            if param == "learning_rate":
                # Learning rate between 0.001 and 0.1
                new_params[param] = max(0.001, min(0.1, new_params[param] * random.uniform(0.5, 2.0)))
            elif param == "energy_decay":
                 # Energy decay between 0.01 and 0.5 # [cite: 2903]
                new_params[param] = max(0.01, min(0.5, new_params[param] * random.uniform(0.7, 1.3)))
            elif param == "base_energy_gain":
                # Base energy gain between 0.5 and 2.0
                new_params[param] = max(0.5, min(2.0, new_params[param] * random.uniform(0.8, 1.2)))
            elif param == "mutation_rate": # [cite: 2904]
                # Mutation rate between 0.001 and 0.01
                new_params[param] = max(0.001, min(0.01, new_params[param] * random.uniform(0.5, 1.5)))
            elif param == "energy_scaling":
                # Energy scaling between 0.5 and 2.0
                new_params[param] = max(0.5, min(2.0, new_params[param] * random.uniform(0.8, 1.2))) # [cite: 2905]
            elif param == "adaptation_rate":
                # Adaptation rate between 0.01 and 0.1
                new_params[param] = max(0.01, min(0.1, new_params[param] * random.uniform(0.7, 1.3)))
            elif param == "cooperation_bonus":
                 # Cooperation bonus between 0.05 and 0.3 # [cite: 2906]
                new_params[param] = max(0.05, min(0.3, new_params[param] * random.uniform(0.8, 1.2)))
            elif param == "specialization_reward":
                # Specialization reward between 0.05 and 0.2
                new_params[param] = max(0.05, min(0.2, new_params[param] * random.uniform(0.8, 1.2)))

        # Create a new CoreLaws instance with the mutated parameters # [cite: 2907]
        return CoreLaws(
            learning_rate=new_params["learning_rate"],
            energy_decay=new_params["energy_decay"],
            base_energy_gain=new_params["base_energy_gain"],
            mutation_rate=new_params["mutation_rate"],
            energy_scaling=new_params["energy_scaling"],
            adaptation_rate=new_params["adaptation_rate"], # [cite: 2908]
            cooperation_bonus=new_params["cooperation_bonus"],
            specialization_reward=new_params["specialization_reward"]
        )

    def get_learning_rate(self) -> float:
        """Returns the current learning rate."""
        return self.learning_rate

    def get_stats(self) -> dict:
        """Returns the operational statistics for this laws instance."""
        return self.stats.copy() # [cite: 2909]

    def reset_stats(self):
        """Resets the operational statistics."""
        for key in self.stats:
            self.stats[key] = 0.0

    def __repr__(self):
        return (f"CoreLaws(id={self.law_id}, learning_rate={self.learning_rate:.3f}, "
                f"energy_decay={self.energy_decay:.3f}, "
                f"base_energy_gain={self.base_energy_gain:.3f}, " # [cite: 2910]
                f"mutation_rate={self.mutation_rate:.4f})")

# (Example usage block from laws.txt [cite: 2911] can be omitted in the final script unless needed for testing)
4. src/llm/client.py (Updated)
(Using the version from llm_client.txtand ensuring imports from logging_config)

Python

# src/llm/client.py
import os
import json
import logging
import asyncio
import subprocess
import requests
import time
import hashlib
import aiohttp # [cite: 2912]
import sys
from typing import Dict, List, Any, Optional, Union, Tuple
import numpy as np
from pathlib import Path

# Use the new logging configuration
from src.utils.logging_config import get_logger

logger = get_logger(__name__)

class LLMClient:
    """
    Advanced LLM client with multi-provider support and adaptive optimization.
    Handles local models through Ollama/llama.cpp and remote APIs with
    automatic batching, caching, and fallback mechanisms. # [cite: 2913]
    """

    def __init__(self,
                 config_file: str = None,
                 api_key: str = None,
                 model: str = "mixtral:latest", # Changed default model
                 provider: str = "ollama",
                 endpoint: str = None, # [cite: 2914]
                 cache_dir: str = ".llm_cache",
                 max_tokens: int = 2048,
                 temperature: float = 0.7,
                 request_timeout: int = 60):
        """
        Initialize the LLM client with adaptive configuration. # [cite: 2915]

        Args: # [cite: 2916]
            config_file: Path to JSON configuration file (optional)
            api_key: API key for provider (if using remote API)
            model: Default model to use
            provider: 'ollama', 'llama.cpp', 'openai', 'anthropic', 'custom'
            endpoint: Custom API endpoint URL
            cache_dir: Directory for response caching # [cite: 2917]
            max_tokens: Default maximum tokens for completion
            temperature: Default temperature for generation
            request_timeout: Timeout for requests in seconds
        """
        self.config = {
            "api_key": api_key,
            "model": model,
            "provider": provider, # [cite: 2918]
            "endpoint": endpoint,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "request_timeout": request_timeout
        }

        # Load config file if provided
        if config_file and os.path.exists(config_file): # [cite: 2919]
            try:
                with open(config_file, 'r') as f:
                    file_config = json.load(f)
                    # Update config with file values, but preserve any explicitly passed values
                    for k, v in file_config.items(): # [cite: 2920]
                        if k not in self.config or self.config[k] is None:
                            self.config[k] = v
                logger.info(f"Loaded LLM configuration from {config_file}")
            except Exception as e: # [cite: 2921]
                logger.error(f"Error loading config from {config_file}: {e}")

        # Environment variable override for sensitive info
        if not self.config["api_key"] and "LLM_API_KEY" in os.environ:
            self.config["api_key"] = os.environ.get("LLM_API_KEY")

        # Setup caching
        self.cache_dir = Path(cache_dir) # [cite: 2922]
        self.cache_dir.mkdir(exist_ok=True, parents=True)
        self.cache_hits = 0

        # Set up provider-specific configuration
        self._setup_provider()

        # Advanced optimization features
        self.request_queue = asyncio.Queue()
        self.batch_size = 5  # Default batch size for request batching # [cite: 2923]
        self.active_requests = 0
        self.is_processing = False
        self.request_semaphore = asyncio.Semaphore(10)  # Limit concurrent requests

        # Adaptive processing - tracks model performance to optimize parameters
        self.perf_metrics = {
            "avg_latency": 0,
            "success_rate": 1.0,
            "total_requests": 0, # [cite: 2924]
            "failed_requests": 0,
            "total_tokens": 0
        }

        logger.info(f"LLMClient initialized with provider: {self.config['provider']}, model: {self.config['model']}")

    def _setup_provider(self):
        """Configure the specific LLM provider."""
        provider = self.config["provider"].lower()

        if provider == "ollama": # [cite: 2925]
            # Validate Ollama availability
            try:
                # Check if Ollama is running and model is available
                endpoint = self.config.get("endpoint", "http://localhost:11434")
                response = requests.get(f"{endpoint}/api/tags", timeout=5) # Added timeout # [cite: 2926]
                if response.status_code != 200:
                    logger.warning(f"Ollama API not responding at {endpoint}. Will attempt to start as needed.") # [cite: 2927]
                else:
                    models = response.json().get("models", [])
                    model_names = [m.get("name") for m in models]
                    if self.config["model"] not in model_names:
                         logger.warning(f"Model {self.config['model']} not found in Ollama. Will attempt to pull on first use.") # [cite: 2928]
            except Exception as e:
                logger.warning(f"Cannot connect to Ollama: {e}. Will attempt to start when needed.")

        elif provider == "llama.cpp":
             # Check if llama.cpp is installed # [cite: 2929]
            try:
                result = subprocess.run(["llama-cli", "--version"],
                                       capture_output=True, text=True, check=False)
                if result.returncode != 0:
                     logger.warning("llama.cpp CLI not found in PATH. Specify model path in config.") # [cite: 2930]
            except Exception as e:
                logger.warning(f"Error checking llama.cpp installation: {e}")

        elif provider in ["openai", "anthropic", "custom"]:
            # Validate API key for remote providers # [cite: 2931]
            if not self.config.get("api_key") and provider != "custom":
                logger.warning(f"No API key provided for {provider}. API calls will likely fail.") # [cite: 2932]

            # Set default endpoints if not specified
            if not self.config.get("endpoint"):
                if provider == "openai":
                    self.config["endpoint"] = "https://api.openai.com/v1/chat/completions"
                elif provider == "anthropic": # [cite: 2933]
                    self.config["endpoint"] = "https://api.anthropic.com/v1/messages"

        else:
            logger.warning(f"Unknown provider: {provider}. Falling back to ollama.")
            self.config["provider"] = "ollama"

    def _get_cache_key(self, prompt: str, system_message: str = None, **kwargs) -> str:
        """Generate a deterministic cache key based on input parameters.""" # [cite: 2934]
        # Create a normalized representation of the request
        cache_dict = {
            "prompt": prompt,
            "system_message": system_message,
            "model": kwargs.get("model", self.config["model"]),
            "temperature": kwargs.get("temperature", self.config["temperature"]),
            "max_tokens": kwargs.get("max_tokens", self.config["max_tokens"]), # [cite: 2935]
            # Add other parameters that affect output
            "stop_sequences": str(kwargs.get("stop_sequences", [])),
            "top_p": kwargs.get("top_p", 1.0),
        }

        # Convert to a consistent string representation and hash
        cache_str = json.dumps(cache_dict, sort_keys=True)
        return hashlib.md5(cache_str.encode()).hexdigest() # [cite: 2936]

    def _check_cache(self, cache_key: str) -> Optional[str]:
        """Check if a response is cached and return it if found."""
        cache_file = self.cache_dir / f"{cache_key}.json"
        if cache_file.exists():
            try:
                with open(cache_file, 'r') as f:
                    cached_data = json.load(f) # [cite: 2937]
                    # Check if cache is still valid (e.g., has expiry)
                    if cached_data.get("expiry", float('inf')) > time.time():
                        self.cache_hits += 1
                        logger.debug(f"Cache hit for key {cache_key}") # [cite: 2938]
                        return cached_data.get("response")
            except Exception as e:
                logger.error(f"Error reading cache file {cache_file}: {e}")

        return None

    def _save_to_cache(self, cache_key: str, response: str, ttl_seconds: int = 86400):
        """Save a response to cache with optional TTL.""" # [cite: 2939]
        cache_file = self.cache_dir / f"{cache_key}.json"
        try:
            with open(cache_file, 'w') as f:
                json.dump({
                    "response": response,
                    "created": time.time(), # [cite: 2940]
                    "expiry": time.time() + ttl_seconds
                }, f)
            logger.debug(f"Saved response to cache: {cache_key}")
        except Exception as e:
            logger.error(f"Error saving to cache file {cache_file}: {e}")

    async def complete(self, # [cite: 2941]
                      prompt: str,
                      system_message: str = None,
                      use_cache: bool = True,
                      **kwargs) -> str:
        """
        Generate a completion for the given prompt. # [cite: 2942]

        Args: # [cite: 2943]
            prompt: The prompt text
            system_message: Optional system message for context
            use_cache: Whether to use response caching
            **kwargs: Override default parameters

        Returns:
            The generated completion text
        """ # [cite: 2944]
        # Override default config with kwargs
        local_config = self.config.copy() # Create local copy for this request
        for k, v in kwargs.items():
            if k in local_config: # Update only known config keys
                local_config[k] = v

        # Check cache if enabled # [cite: 2945]
        if use_cache:
            cache_key = self._get_cache_key(prompt, system_message, **local_config) # Use local_config for key
            cached_response = self._check_cache(cache_key)
            if cached_response:
                return cached_response

        # Track metrics # [cite: 2946]
        start_time = time.time()
        self.perf_metrics["total_requests"] += 1

        try:
            # Call the appropriate provider method using local_config
            provider = local_config["provider"].lower()

            if provider == "ollama": # [cite: 2947]
                response = await self._complete_ollama(prompt, system_message, **local_config)
            elif provider == "llama.cpp":
                response = await self._complete_llamacpp(prompt, system_message, **local_config)
            elif provider == "openai":
                response = await self._complete_openai(prompt, system_message, **local_config)
            elif provider == "anthropic": # [cite: 2948]
                response = await self._complete_anthropic(prompt, system_message, **local_config)
            elif provider == "custom":
                response = await self._complete_custom(prompt, system_message, **local_config)
            else:
                # Fallback to Ollama
                logger.warning(f"Unknown provider {provider}, falling back to Ollama") # [cite: 2949]
                local_config["provider"] = "ollama" # Update local config for fallback
                response = await self._complete_ollama(prompt, system_message, **local_config)

            # Cache the successful response
            if use_cache and 'cache_key' in locals(): # Ensure cache_key was generated # [cite: 2950]
                self._save_to_cache(cache_key, response)

            # Update performance metrics
            latency = time.time() - start_time
            # Use running average calculation
            n = self.perf_metrics["total_requests"]
            self.perf_metrics["avg_latency"] = ((n - 1) * self.perf_metrics["avg_latency"] + latency) / n # [cite: 2951]

            # Estimate token count (very rough approximation)
            self.perf_metrics["total_tokens"] += self.estimate_token_count(response) # Use helper method

            return response # [cite: 2952]

        except Exception as e:
            self.perf_metrics["failed_requests"] += 1
            # Recalculate success rate carefully to avoid division by zero
            if self.perf_metrics["total_requests"] > 0:
                self.perf_metrics["success_rate"] = 1.0 - (self.perf_metrics["failed_requests"] /
                                                        self.perf_metrics["total_requests"]) # [cite: 2953]
            else:
                 self.perf_metrics["success_rate"] = 0.0

            logger.error(f"Error completing prompt: {e}", exc_info=True)

            # Implement fallback mechanism for robustness
            if local_config.get("fallback", True) and provider != "ollama": # Check local_config for fallback flag # [cite: 2954]
                 logger.info(f"Attempting fallback to Ollama for failed request")
                 try:
                     fallback_config = local_config.copy()
                     fallback_config["provider"] = "ollama"
                     fallback_config["fallback"] = False  # Prevent infinite fallback loops
                     return await self.complete(prompt, system_message, use_cache, **fallback_config) # Pass fallback_config # [cite: 2955]
                 except Exception as fallback_e:
                     logger.error(f"Fallback also failed: {fallback_e}")

            raise RuntimeError(f"LLM completion failed: {str(e)}")

    async def _complete_ollama(self, prompt: str, system_message: str = None, **kwargs) -> str:
        """Generate completion using Ollama API.""" # [cite: 2956]
        # Use kwargs directly as they represent the request-specific config
        endpoint = kwargs.get("endpoint", "http://localhost:11434")
        model = kwargs.get("model")
        max_tokens = kwargs.get("max_tokens")
        temperature = kwargs.get("temperature")
        stop_sequences = kwargs.get("stop_sequences", [])
        timeout = kwargs.get("request_timeout")

        # Check if Ollama is running, start it if needed # [cite: 2957]
        try:
            async with aiohttp.ClientSession() as session:
                # Use a short timeout for the check
                async with session.get(f"{endpoint}/api/tags", timeout=aiohttp.ClientTimeout(total=5)) as response:
                    if response.status != 200:
                        # Attempt to start Ollama # [cite: 2958]
                        logger.info("Ollama not running, attempting to start...")
                        # Use Popen to avoid blocking, manage process lifecycle if needed
                        subprocess.Popen(["ollama", "serve"],
                                        stdout=subprocess.DEVNULL, # Redirect output
                                        stderr=subprocess.DEVNULL) # [cite: 2959]
                        # Wait for service to start
                        for _ in range(10): # Increased wait time
                            await asyncio.sleep(2) # Wait longer # [cite: 2960]
                            try:
                                # Check again
                                async with session.get(f"{endpoint}/api/tags", timeout=aiohttp.ClientTimeout(total=2)) as check_response: # [cite: 2961]
                                    if check_response.status == 200:
                                        logger.info("Ollama started successfully")
                                        break # [cite: 2962]
                            except (aiohttp.ClientError, asyncio.TimeoutError):
                                pass
                        else:
                            logger.error("Failed to start or connect to Ollama after multiple attempts") # [cite: 2963]
                            # Optionally raise an error here if Ollama is essential
                            # raise RuntimeError("Failed to start Ollama")
        except (aiohttp.ClientError, asyncio.TimeoutError) as e:
            logger.warning(f"Error checking/starting Ollama: {e}")

        # Prepare the request
        request_url = f"{endpoint}/api/generate"

        request_body = { # [cite: 2964]
            "model": model,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": temperature,
                "num_predict": max_tokens
            } # [cite: 2965]
        }

        if system_message:
            request_body["system"] = system_message

        if stop_sequences:
            request_body["options"]["stop"] = stop_sequences

        # Make the API request
        try: # [cite: 2966]
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=timeout)) as session:
                async with self.request_semaphore: # Use semaphore
                    async with session.post(request_url, json=request_body) as response:
                        if response.status != 200:
                            error_text = await response.text() # [cite: 2967]

                            # Check if model needs to be pulled
                            if "model not found" in error_text.lower(): # [cite: 2968]
                                logger.info(f"Model {model} not found, attempting to pull...")
                                # Use asyncio.create_subprocess_exec for non-blocking pull
                                pull_process = await asyncio.create_subprocess_exec(
                                    "ollama", "pull", model, # [cite: 2969]
                                    stdout=asyncio.subprocess.PIPE,
                                    stderr=asyncio.subprocess.PIPE
                                ) # [cite: 2970]
                                stdout, stderr = await pull_process.communicate()

                                if pull_process.returncode == 0: # [cite: 2971]
                                    logger.info(f"Successfully pulled model {model}, retrying request")
                                    # Retry the request
                                    async with session.post(request_url, json=request_body) as retry_response: # [cite: 2972]
                                        if retry_response.status == 200:
                                            response_json = await retry_response.json() # [cite: 2973]
                                            return response_json.get("response", "")
                                        else:
                                            error_text = await retry_response.text() # [cite: 2974]
                                            raise RuntimeError(f"Ollama API error after model pull: {error_text}") # [cite: 2975]
                                else:
                                    raise RuntimeError(f"Failed to pull model {model}: {stderr.decode()}") # [cite: 2976]
                            else:
                                raise RuntimeError(f"Ollama API error: {error_text}")

                        response_json = await response.json() # [cite: 2978]
                        return response_json.get("response", "")

        except asyncio.TimeoutError:
            raise RuntimeError(f"Ollama request timed out after {timeout} seconds")
        except Exception as e:
            raise RuntimeError(f"Error with Ollama request: {str(e)}") # [cite: 2979]

    # --- Implementations for _complete_llamacpp, _complete_openai, _complete_anthropic, _complete_custom remain similar ---
    # ... (Keep existing implementationsor refine as needed) ...
    async def _complete_llamacpp(self, prompt: str, system_message: str = None, **kwargs) -> str:
        """Generate completion using llama.cpp CLI."""
        model_path = kwargs.get("model_path", self.config.get("model_path", "models/7B/ggml-model.bin"))
        max_tokens = kwargs.get("max_tokens", self.config["max_tokens"])
        temperature = kwargs.get("temperature", self.config["temperature"])
        stop_sequences = kwargs.get("stop_sequences", [])

        # Build the command # [cite: 2980]
        cmd = ["llama-cli", "--model", model_path,
               "--temp", str(temperature),
               "--n-predict", str(max_tokens),
               "--silent-prompt"]

        # Add stop sequences if provided
        for stop in stop_sequences:
            cmd.extend(["--reverse-prompt", stop]) # [cite: 2981]

        # Prepare prompt with system message if provided
        full_prompt = prompt
        if system_message:
            full_prompt = f"{system_message}\\n\\n{prompt}"

        # Run the command
        try:
            process = await asyncio.create_subprocess_exec( # [cite: 2982]
                *cmd,
                stdin=asyncio.subprocess.PIPE,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )

            # Send prompt and get response # [cite: 2983]
            stdout, stderr = await process.communicate(full_prompt.encode())

            if process.returncode != 0:
                raise RuntimeError(f"llama.cpp error: {stderr.decode()}")

            # Process the output # [cite: 2984]
            response = stdout.decode().strip()

            # Remove the prompt from the beginning of the response
            if response.startswith(full_prompt):
                response = response[len(full_prompt):].strip()

            return response
        except Exception as e: # [cite: 2985]
            raise RuntimeError(f"Error running llama.cpp: {str(e)}")

    async def _complete_openai(self, prompt: str, system_message: str = None, **kwargs) -> str:
        """Generate completion using OpenAI API."""
        # Use kwargs directly
        api_key = kwargs.get("api_key")
        endpoint = kwargs.get("endpoint") # [cite: 2986]
        model = kwargs.get("model")
        max_tokens = kwargs.get("max_tokens")
        temperature = kwargs.get("temperature")
        stop_sequences = kwargs.get("stop_sequences", [])
        timeout = kwargs.get("request_timeout")

        if not api_key:
            raise ValueError("OpenAI API key not provided")

        # Prepare the messages # [cite: 2987]
        messages = []
        if system_message:
            messages.append({"role": "system", "content": system_message})
        messages.append({"role": "user", "content": prompt})

        # Prepare the request
        headers = { # [cite: 2988]
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}"
        }
        request_body = {
            "model": model,
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": temperature # [cite: 2989]
        }
        if stop_sequences:
            request_body["stop"] = stop_sequences

        # Make the API request
        try:
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=timeout)) as session:
                async with self.request_semaphore: # [cite: 2990]
                    async with session.post(endpoint, headers=headers, json=request_body) as response:
                        if response.status != 200:
                            error_text = await response.text()
                            raise RuntimeError(f"OpenAI API error: {error_text}") # [cite: 2991]
                        response_json = await response.json()
                        # Safely access nested keys
                        choices = response_json.get("choices", [])
                        if choices:
                            first_choice = choices[0]
                            message = first_choice.get("message", {})
                            content = message.get("content", "")
                            return content
                        return "" # Return empty if structure is unexpected
        except asyncio.TimeoutError: # [cite: 2992]
            raise RuntimeError(f"OpenAI request timed out after {timeout} seconds")
        except Exception as e:
            raise RuntimeError(f"Error with OpenAI request: {str(e)}")

    async def _complete_anthropic(self, prompt: str, system_message: str = None, **kwargs) -> str:
        """Generate completion using Anthropic API."""
        # Use kwargs directly
        api_key = kwargs.get("api_key") # [cite: 2993]
        endpoint = kwargs.get("endpoint")
        model = kwargs.get("model")
        max_tokens = kwargs.get("max_tokens")
        temperature = kwargs.get("temperature")
        timeout = kwargs.get("request_timeout")

        if not api_key:
            raise ValueError("Anthropic API key not provided")

        # Prepare the request # [cite: 2994]
        headers = {
            "Content-Type": "application/json",
            "X-API-Key": api_key,
            "anthropic-version": "2023-06-01"
        }
        request_body = {
            "model": model,
            "max_tokens": max_tokens, # [cite: 2995]
            "temperature": temperature,
            "messages": [{"role": "user", "content": prompt}]
        }
        if system_message:
            request_body["system"] = system_message

        # Make the API request
        try: # [cite: 2996]
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=timeout)) as session:
                async with self.request_semaphore:
                    async with session.post(endpoint, headers=headers, json=request_body) as response:
                        if response.status != 200: # [cite: 2997]
                            error_text = await response.text()
                            raise RuntimeError(f"Anthropic API error: {error_text}")
                        response_json = await response.json() # [cite: 2998]
                        # Safely access nested keys
                        content_list = response_json.get("content", [])
                        if content_list:
                            first_content = content_list[0]
                            text = first_content.get("text", "")
                            return text
                        return "" # Return empty if structure is unexpected
        except asyncio.TimeoutError:
            raise RuntimeError(f"Anthropic request timed out after {timeout} seconds")
        except Exception as e: # [cite: 2999]
            raise RuntimeError(f"Error with Anthropic request: {str(e)}")

    async def _complete_custom(self, prompt: str, system_message: str = None, **kwargs) -> str:
        """Generate completion using a custom API endpoint."""
        # Use kwargs directly
        api_key = kwargs.get("api_key")
        endpoint = kwargs.get("endpoint") # [cite: 3000]
        timeout = kwargs.get("request_timeout")

        if not endpoint:
            raise ValueError("Custom API endpoint not provided")

        # Prepare the request - adapt this based on your custom API format
        headers = {}
        if api_key:
            headers["Authorization"] = f"Bearer {api_key}"
        request_body = {
            "prompt": prompt,
            "max_tokens": kwargs.get("max_tokens"), # [cite: 3001]
            "temperature": kwargs.get("temperature")
        }
        if system_message:
            request_body["system"] = system_message

        # Make the API request
        try: # [cite: 3002]
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=timeout)) as session:
                async with self.request_semaphore:
                    async with session.post(endpoint, headers=headers, json=request_body) as response:
                        if response.status != 200:
                            error_text = await response.text() # [cite: 3003]
                            raise RuntimeError(f"Custom API error: {error_text}")
                        response_json = await response.json()
                        # Adapt this based on your API's response format # [cite: 3004]
                        return response_json.get("response", "")
        except asyncio.TimeoutError:
            raise RuntimeError(f"Custom API request timed out after {timeout} seconds")
        except Exception as e:
            raise RuntimeError(f"Error with custom API request: {str(e)}") # [cite: 3005]

    async def batch_process(self, prompts: List[str], system_message: str = None, **kwargs) -> List[str]:
        """
        Process multiple prompts more efficiently as a batch. # [cite: 3006]
        Args:
            prompts: List of prompts to process
            system_message: Optional system message for all prompts
            **kwargs: Override default parameters

        Returns:
            List of completions corresponding to input prompts
        """
        results = [] # [cite: 3007]

        # Process in parallel with concurrency limit
        tasks = []
        for prompt in prompts:
            # Ensure ensure_future is called within the running event loop if this function might be called from sync code
            task = asyncio.create_task(self.complete(prompt, system_message, **kwargs))
            tasks.append(task)

        # Wait for all completions and collect results # [cite: 3008]
        # Using asyncio.gather preserves order and handles exceptions
        task_results = await asyncio.gather(*tasks, return_exceptions=True)

        for i, res in enumerate(task_results):
            if isinstance(res, Exception):
                # Add error message for failed requests # [cite: 3009]
                logger.error(f"Error processing prompt {i} in batch: {res}")
                results.append(f"Error: {str(res)}")
            else:
                results.append(res)

        return results

    def get_performance_stats(self) -> Dict[str, Any]:
        """Get current performance statistics."""
        return {
            "avg_latency_seconds": round(self.perf_metrics["avg_latency"], 2),
            "success_rate": round(self.perf_metrics["success_rate"], 4), # [cite: 3010]
            "total_requests": self.perf_metrics["total_requests"],
            "failed_requests": self.perf_metrics["failed_requests"],
            "estimated_total_tokens": int(self.perf_metrics["total_tokens"]),
            "cache_hits": self.cache_hits,
            "provider": self.config["provider"],
            "model": self.config["model"]
        }

    def estimate_token_count(self, text: str) -> int:
        """Roughly estimate the number of tokens in a text.""" # [cite: 3011]
        # Simple estimation based on words and punctuation
        # More accurate methods would use model-specific tokenizers
        words = len(text.split())
        return int(words * 1.3)  # Rough approximation

# (Example usage block from llm_client.txtcan be omitted in the final script unless needed for testing)
5. src/system_upgrade_module.py (Continued and Completed)
(Completing the PythonModernizer and adding the SystemUpgrader class)

Python

# src/system_upgrade_module.py (Continued)
# ... (Previous imports, Enums, Dataclasses, LanguageDetector, SystemAnalyzer, CodeTransformer base class) ...

class PythonModernizer(CodeTransformer):
    """Modernizes Python code"""

    def can_transform(self, code_file: CodeFile) -> bool:
        """Check if this transformer can handle the given file""" # [cite: 1568]
        return code_file.language == LanguageType.PYTHON

    def transform(self, code_file: CodeFile, system_info: SystemInfo) -> Tuple[str, List[str]]:
        """Transform Python code to modern standards"""
        content = code_file.content
        transformations = []

        # Add type hints
        content, type_transforms = self._add_type_hints(content) # [cite: 1569]
        if type_transforms:
            transformations.append("Added type hints")

        # Convert to f-strings
        content, fstring_count = self._convert_to_fstrings(content)
        if fstring_count > 0:
            transformations.append(f"Converted {fstring_count} string formats to f-strings")

        # Use modern Python features
        content, modern_transforms = self._modernize_python_features(content) # [cite: 1570]
        transformations.extend(modern_transforms)

        # Update imports (placeholder)
        # content, import_transforms = self._update_imports(content, system_info)
        # transformations.extend(import_transforms)

        return content, transformations

    def _add_type_hints(self, content: str) -> Tuple[str, List[str]]:
        """Add type hints to Python code""" # [cite: 1571]
        # Placeholder implementation: just add typing import if missing
        if 'from typing import ' not in content and 'import typing' not in content:
            content = "from typing import List, Dict, Tuple, Optional, Any, Union\n\n" + content
            return content, ["Added basic typing imports"]
        return content, []

    def _convert_to_fstrings(self, content: str) -> Tuple[str, int]: # [cite: 1572]
        """Convert old-style string formatting to f-strings"""
        # Convert .format() style
        pattern_format = r'([\'"](.*?)(?<!\\)[\'"])\s*\.\s*format\s*\((.*?)\)'
        # Convert % style
        pattern_percent = r'([\'"](.*?)(?<!\\)[\'"])\s*%\s*\((.*?)\)'

        count = 0

        def replace_format(match):
            nonlocal count
            quotes = match.group(1)[0] # Detect quote type ' or "
            string_template = match.group(2)
            args_str = match.group(3).strip() # [cite: 1573]

            # Basic heuristic: Convert only if args are simple variables/expressions
            # This avoids complex nested formats or keyword args for simplicity
            # A more robust solution would use AST parsing
            if re.fullmatch(r'([\w\.\(\)\[\]\'"]+(\s*,\s*[\w\.\(\)\[\]\'"]+)*)?', args_str):
                try:
                    # Replace {} placeholders with {arg_name} based on order
                    args = [a.strip() for a in args_str.split(',')]
                    fstring_content = string_template
                    for i, arg in enumerate(args):
                        # Basic placeholder replacement
                        fstring_content = fstring_content.replace(f'{{{i}}}', f'{{{arg}}}', 1)
                        # Also handle implicit placeholders {}
                        fstring_content = re.sub(r'{}', f'{{{arg}}}', fstring_content, count=1)

                    # Check if all placeholders were potentially replaced
                    if '{' not in fstring_content or re.search(r'{\d+}', fstring_content): # Check for remaining numeric placeholders
                         # Fallback if complex placeholders remain
                         return match.group(0)

                    # Escape existing braces inside the string
                    fstring_content = fstring_content.replace('{', '{{').replace('}', '}}')
                    # Unescape the placeholders we just added
                    fstring_content = fstring_content.replace('{{', '{').replace('}}', '}')

                    new_str = f"f{quotes}{fstring_content}{quotes}"
                    count += 1
                    return new_str
                except Exception as e:
                    logger.debug(f"Skipping f-string conversion for complex format: {match.group(0)} due to {e}")
                    return match.group(0) # Skip complex cases
            return match.group(0)

        def replace_percent(match):
            nonlocal count
            quotes = match.group(1)[0]
            string_template = match.group(2)
            args_str = match.group(3).strip()

            # Basic heuristic for simple named placeholders %(name)s
            placeholders = re.findall(r'%\((\w+)\)[sdif]', string_template)
            if placeholders and re.fullmatch(r'([\w\s\'"]+\s*:\s*[\w\.\(\)\[\]\'"]+(\s*,\s*[\w\s\'"]+\s*:\s*[\w\.\(\)\[\]\'"]+)*)?', args_str):
                 try:
                    fstring_content = re.sub(r'%\((\w+)\)[sdif]', r'{\1}', string_template)
                     # Escape existing braces
                    fstring_content = fstring_content.replace('{', '{{').replace('}', '}}')
                    # Unescape placeholders
                    fstring_content = fstring_content.replace('{{', '{').replace('}}', '}')

                    new_str = f"f{quotes}{fstring_content}{quotes}"
                    count += 1
                    return new_str
                 except Exception as e:
                    logger.debug(f"Skipping f-string conversion for complex % format: {match.group(0)} due to {e}")
                    return match.group(0)
            return match.group(0) # Skip complex or non-named cases

        # Apply replacements
        content = re.sub(pattern_format, replace_format, content)
        content = re.sub(pattern_percent, replace_percent, content)

        return content, count # [cite: 1687]

    def _modernize_python_features(self, content: str) -> Tuple[str, List[str]]: # [cite: 1684]
        transformations = []
        # Example: Replace print statements with function calls (Python 2 -> 3)
        if re.search(r'^print\s+[^(\n]', content, re.MULTILINE):
            content = re.sub(r'^print\s+(.+)$', r'print(\1)', content, flags=re.MULTILINE)
            transformations.append("Converted print statements to Python 3 style")
        # Add more modernizations here (e.g., using pathlib, context managers)
        return content, transformations

    def _update_imports(self, content: str, system_info: SystemInfo) -> Tuple[str, List[str]]:
        # Placeholder: Could use system_info.dependencies to update versions or imports
        # Needs a dependency analysis tool integration (e.g., pip-tools, safety)
        return content, []

# --- Add other CodeTransformer implementations (JavaScriptModernizer, etc.) ---
# Placeholder for other languages
class JavaScriptModernizer(CodeTransformer):
    def can_transform(self, code_file: CodeFile) -> bool:
        return code_file.language == LanguageType.JAVASCRIPT

    def transform(self, code_file: CodeFile, system_info: SystemInfo) -> Tuple[str, List[str]]:
        logger.warning(f"JavaScript modernization not fully implemented for {code_file.path}")
        # Add basic transformations (e.g., var -> let/const)
        content = code_file.content
        # Simple example: replace 'var' with 'let' in appropriate scopes (needs AST for correctness)
        # content = re.sub(r'\bvar\b', 'let', content)
        return content, ["JavaScript Modernization (Placeholder)"]

# (Add similar placeholders for JavaModernizer, CppModernizer etc. if needed)

# --- SystemUpgrader Class (Integrates Analysis and Transformation) ---
class SystemUpgrader:
    """Orchestrates the system upgrade process"""

    def __init__(self):
        self.analyzer = SystemAnalyzer()
        # Register available transformers
        self.transformers: List[CodeTransformer] = [
            PythonModernizer(),
            JavaScriptModernizer(),
            # Add other transformers here
        ]
        self.logger = logging.getLogger(__name__) # [cite: 1753]

    def upgrade_system(self, root_path: str, config: UpgradeConfig) -> Dict[str, Any]:
        """Performs the system upgrade based on the configuration"""
        start_time = datetime.datetime.now()
        self.logger.info(f"Starting system upgrade for {root_path} with strategy {config.strategy.name}")

        try:
            # 1. Analyze the system
            system_info = self.analyzer.analyze_system(root_path) # [cite: 1754]

            # 2. Create backup if requested
            backup_path = None
            if config.keep_original:
                backup_path = self._create_backup(root_path)
                self.logger.info(f"Created backup at {backup_path}")

            # 3. Prepare output directory
            # Using a temporary directory first, then copy to final location if successful
            with tempfile.TemporaryDirectory(prefix="kaleidoscope_upgrade_") as temp_output_dir:
                output_path = temp_output_dir
                self.logger.info(f"Using temporary output directory: {output_path}")

                upgraded_files = []
                errors = []
                all_transformations: Dict[str, List[str]] = {} # File path -> List of transforms

                # 4. Process and Transform files (potentially in parallel)
                # Create a list of files to process
                files_to_process = list(system_info.files.items())

                # Use multiprocessing for parallel transformation
                # Note: Requires careful handling of shared state (SystemInfo) if modified by transformers
                # For simplicity here, assume transformers primarily work on content
                pool = multiprocessing.Pool(processes=config.max_parallel_processes)
                results = pool.starmap(self._transform_file_wrapper,
                                       [(file_path, code_file, system_info, config) for file_path, code_file in files_to_process])
                pool.close()
                pool.join()

                # Collect results
                for file_path, new_content, file_transforms, error in results:
                    if error:
                        errors.append(f"Failed to transform {file_path}: {error}") # [cite: 1758]
                    else:
                        # Write transformed file to output directory
                        output_file = os.path.join(output_path, file_path)
                        os.makedirs(os.path.dirname(output_file), exist_ok=True)
                        with open(output_file, 'w', encoding='utf-8') as f:
                            f.write(new_content) # [cite: 1757]
                        upgraded_files.append(file_path)
                        if file_transforms:
                            all_transformations[file_path] = file_transforms

                # 5. Update Dependency Files (Example for Python requirements.txt)
                if config.update_dependencies and system_info.primary_language == LanguageType.PYTHON:
                    req_path = os.path.join(root_path, "requirements.txt")
                    if os.path.exists(req_path):
                        try:
                            updated_req = self._update_python_dependencies(req_path)
                            output_req_path = os.path.join(output_path, "requirements.txt")
                            with open(output_req_path, 'w') as f:
                                f.write(updated_req)
                            all_transformations["requirements.txt"] = ["Updated dependencies"]
                        except Exception as dep_e:
                            errors.append(f"Failed to update dependencies: {dep_e}")

                # 6. Copy non-transformed files
                self._copy_other_files(root_path, output_path, system_info, config.excluded_paths)

                # 7. Finalize Results
                time_taken = (datetime.datetime.now() - start_time).total_seconds()
                size_diff = self._calculate_size_difference(root_path, output_path)
                final_output_path = f"{root_path}_upgraded_{uuid.uuid4().hex[:8]}"
                shutil.move(output_path, final_output_path) # Move from temp to final location

                upgrade_result = UpgradeResult(
                    success=len(errors) == 0,
                    output_path=final_output_path, # Use the final path
                    strategy_used=config.strategy, # [cite: 1759]
                    upgraded_files=upgraded_files,
                    errors=errors,
                    backup_path=backup_path,
                    time_taken_seconds=time_taken,
                    size_difference=size_diff, # [cite: 1760]
                    applied_transformations=[f"{f}: {', '.join(t)}" for f, t in all_transformations.items()]
                ) # [cite: 1805]

                self.logger.info(f"Upgrade process finished in {time_taken:.2f}s. Success: {upgrade_result.success}")
                return upgrade_result.__dict__

        except Exception as e:
            self.logger.error(f"System upgrade failed: {str(e)}", exc_info=True)
            return UpgradeResult(
                success=False,
                output_path=root_path, # No new output path created
                strategy_used=config.strategy,
                errors=[f"Critical error during upgrade: {str(e)}"],
                backup_path=backup_path,
                time_taken_seconds=(datetime.datetime.now() - start_time).total_seconds()
            ).__dict__

    def _transform_file_wrapper(self, file_path: str, code_file: CodeFile, system_info: SystemInfo, config: UpgradeConfig):
        """Wrapper function for parallel file transformation"""
        try:
            transformer = next((t for t in self.transformers if t.can_transform(code_file)), None)
            if transformer and config.refactor_code: # Check if refactoring is enabled
                # Add logic here to potentially use LLM service for refactoring/modernization
                # if config.use_llm and self.llm_service: ...
                new_content, file_transforms = transformer.transform(code_file, system_info)
                return file_path, new_content, file_transforms, None
            else:
                # If no transformer or refactoring disabled, return original content
                return file_path, code_file.content, [], None
        except Exception as e:
            self.logger.error(f"Error transforming {file_path} in worker: {e}")
            return file_path, code_file.content, [], str(e)


    def _create_backup(self, root_path: str) -> str:
        """Creates a backup of the original system"""
        backup_dir = f"{root_path}_backup_{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}" # More descriptive name
        try:
            shutil.copytree(root_path, backup_dir, ignore=shutil.ignore_patterns('.git*', 'node_modules*', '*__pycache__*'))
            self.logger.info(f"Backup created at {backup_dir}")
            return backup_dir
        except Exception as e:
            self.logger.error(f"Failed to create backup: {e}")
            return None # [cite: 1760]

    def _calculate_size_difference(self, original_path: str, new_path: str) -> int:
        """Calculates the size difference between original and new paths"""
        try:
            orig_size = sum(p.stat().st_size for p in Path(original_path).rglob('*') if p.is_file())
            new_size = sum(p.stat().st_size for p in Path(new_path).rglob('*') if p.is_file())
            return new_size - orig_size
        except Exception as e:
            self.logger.error(f"Could not calculate size difference: {e}")
            return 0 # [cite: 1812]

    def _copy_other_files(self, src_dir, dest_dir, system_info: SystemInfo, excluded_paths: List[str]):
         """Copies non-code files and excluded files to the destination."""
         for root, dirs, files in os.walk(src_dir, topdown=True):
             # Skip excluded directories
             dirs[:] = [d for d in dirs if d not in self.excluded_dirs and os.path.join(root, d) not in excluded_paths]

             for file in files:
                 src_file_path = os.path.join(root, file)
                 rel_path = os.path.relpath(src_file_path, src_dir)

                 # Skip if it's an analyzed code file or explicitly excluded
                 if rel_path in system_info.files or any(rel_path.startswith(ex) for ex in excluded_paths):
                     continue

                 dest_file_path = os.path.join(dest_dir, rel_path)
                 os.makedirs(os.path.dirname(dest_file_path), exist_ok=True)
                 try:
                     shutil.copy2(src_file_path, dest_file_path)
                 except Exception as e:
                     self.logger.warning(f"Could not copy file {src_file_path}: {e}")

    def _update_python_dependencies(self, requirements_path: str) -> str:
        """Updates Python dependencies using pip-tools (requires installation)"""
        # Placeholder: Requires pip-compile or similar tool
        # Example using pip freeze (less accurate for updates)
        try:
            # Activate venv if necessary
            if sys.prefix == sys.base_prefix:
                 # Attempt to find venv Python
                 venv_python = Path.cwd() / "venv" / ("Scripts" if sys.platform == 'win32' else "bin") / "python"
                 if venv_python.exists():
                     result = subprocess.run([str(venv_python), "-m", "pip", "freeze"], capture_output=True, text=True, check=True)
                     return result.stdout
                 else:
                     logger.warning("Cannot update requirements: Not in venv and venv Python not found.")
                     with open(requirements_path, 'r') as f: return f.read() # Return original
            else:
                 # Already in venv
                 result = subprocess.run([sys.executable, "-m", "pip", "freeze"], capture_output=True, text=True, check=True)
                 return result.stdout
        except Exception as e:
            logger.error(f"Failed to update Python dependencies: {e}")
            with open(requirements_path, 'r') as f: return f.read() # Return original on error

# (Remaining classes like PythonModernizer need completing)
# ... (Implementation of PythonModernizer._modernize_python_features, _update_imports)
# ... (Implementations for other CodeTransformer classes: JavaScriptModernizer, etc.)

# --- Main Execution Block (Example) ---
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Kaleidoscope AI - System Upgrade Module")
    parser.add_argument("path", help="Root path of the software system to upgrade")
    parser.add_argument("--target", default="python", help="Target language (e.g., python, javascript)")
    parser.add_argument("--strategy", default="in_place", help="Upgrade strategy (e.g., in_place, full_rewrite)")
    args = parser.parse_args()

    config = UpgradeConfig(
        target_language=LanguageType[args.target.upper()],
        strategy=UpgradeStrategy[args.strategy.upper()]
    )

    upgrader = SystemUpgrader()
    result = upgrader.upgrade_system(args.path, config)

    print("\n--- Upgrade Result ---")
    print(f"Success: {result['success']}")
    print(f"Output Path: {result['output_path']}")
    print(f"Backup Path: {result['backup_path']}")
    print(f"Time Taken: {result['time_taken_seconds']:.2f}s")
    print(f"Errors: {len(result['errors'])}")
    if result['errors']:
        for err in result['errors'][:5]: print(f" - {err}")
Next Steps:

Complete PythonModernizer: Finish the _modernize_python_features and _update_imports methods.
Add More Transformers: Implement JavaScriptModernizer, JavaModernizer, etc.
Integrate unravel_ai_core_engine: Use FileAnalyzer in SystemAnalyzer, potentially use Decompiler outputs.
Refine kaleidoscope_main.py: Integrate the SystemUpgrader workflow.
Address Missing Core Classes: Implement CoreLaws, CapabilityNode, etc.
I'll pause here. Please reply with "k" to continue with the next set of integrations.


Sources and related content
system

TXT
systemfixe...additions3

TXT
systemadditions

TXT
error_handler

TXT
error_handler (1)

TXT




k



Show thinking
Okay, continuing from where we left off with src/system_upgrade_module.py. I'll complete the PythonModernizer class and add the SystemUpgrader implementation.

5. src/system_upgrade_module.py (Completed)
(Finishing PythonModernizer and adding SystemUpgrader, placeholders for other transformers)

Python

# src/system_upgrade_module.py (Continued and Completed)
import os
import sys
import re
import ast
import json
import shutil
import tempfile
import subprocess
import importlib
import logging
import zipfile
import tarfile
import uuid
import hashlib
import datetime
import docker # Note: Imported but not used in this specific code - potentially for future containerized upgrades
import multiprocessing
from pathlib import Path
from typing import Dict, List, Set, Tuple, Any, Optional, Union, Callable
from dataclasses import dataclass, field
from enum import Enum, auto
from abc import ABC, abstractmethod
import networkx as nx

# Setup logging using the configuration utility
# Assuming src is in the Python path or adjust relative import
try:
    from src.utils.logging_config import get_logger
except ImportError:
    # Fallback if running standalone or structure differs
    logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(name)s: %(message)s')
    logger = logging.getLogger(__name__)
    def get_logger(name): return logging.getLogger(name)

logger = get_logger(__name__)

# --- Enums ---
class LanguageType(Enum):
    """Supported programming languages"""
    PYTHON = auto() #
    JAVASCRIPT = auto()
    TYPESCRIPT = auto()
    JAVA = auto()
    CSHARP = auto()
    CPP = auto()
    RUBY = auto()
    PHP = auto()
    GO = auto()
    RUST = auto()
    SWIFT = auto()
    KOTLIN = auto()
    UNKNOWN = auto()

class SystemType(Enum):
    """Types of systems to upgrade"""
    WEB_APP = auto() #
    DESKTOP_APP = auto()
    MOBILE_APP = auto() #
    API = auto()
    CLI = auto()
    LIBRARY = auto()
    FRAMEWORK = auto()
    DATABASE = auto()
    UNKNOWN = auto()

class UpgradeStrategy(Enum):
    """Strategies for system upgrades"""
    IN_PLACE = auto()  # Modify existing codebase
    INCREMENTAL = auto()  # Upgrade component by component
    FULL_REWRITE = auto()  # Complete rewrite with same language
    LANGUAGE_MIGRATION = auto()  # Rewrite in different language #
    WRAPPER = auto()  # Create wrapper around existing system #

# --- Dataclasses ---
@dataclass
class DependencyInfo:
    """Information about a dependency"""
    name: str
    version: str
    current_version: Optional[str] = None
    latest_version: Optional[str] = None
    is_vulnerable: bool = False
    vulnerability_details: Optional[str] = None
    is_outdated: bool = False
    upgrade_path: Optional[str] = None
    is_deprecated: bool = False
    alternatives: List[str] = field(default_factory=list) # Fixed default factory

@dataclass
class CodeFile:
    """Information about a code file"""
    path: str
    language: LanguageType #
    content: str
    ast: Optional[Any] = None
    imports: List[str] = field(default_factory=list) # Fixed default factory
    exports: List[str] = field(default_factory=list) # Fixed default factory
    dependencies: List[DependencyInfo] = field(default_factory=list) # Fixed default factory
    vulnerabilities: List[str] = field(default_factory=list) # Fixed default factory
    outdated_patterns: List[str] = field(default_factory=list) # Fixed default factory
    complexity_score: float = 0.0
    is_test: bool = False

@dataclass
class SystemInfo:
    """Information about the system to upgrade"""
    root_path: str
    system_type: SystemType
    primary_language: LanguageType
    other_languages: List[LanguageType] = field(default_factory=list) # Fixed default factory
    files: Dict[str, CodeFile] = field(default_factory=dict) # Fixed default factory
    dependencies: Dict[str, DependencyInfo] = field(default_factory=dict) # Fixed default factory
    entry_points: List[str] = field(default_factory=list) # Fixed default factory
    config_files: List[str] = field(default_factory=list) # Fixed default factory
    database_info: Dict[str, Any] = field(default_factory=dict) # Fixed default factory
    api_endpoints: List[str] = field(default_factory=list) # Fixed default factory
    vulnerabilities: List[str] = field(default_factory=list) # Fixed default factory
    dependencies_graph: Optional[nx.DiGraph] = None
    file_count: int = 0
    code_size: int = 0  # In bytes

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary"""
        result = { #
            "root_path": self.root_path,
            "system_type": self.system_type.name,
            "primary_language": self.primary_language.name,
            "other_languages": [lang.name for lang in self.other_languages],
            "entry_points": self.entry_points,
            "config_files": self.config_files,
            "database_info": self.database_info,
            "api_endpoints": self.api_endpoints, #
            "vulnerabilities": self.vulnerabilities,
            "file_count": self.file_count,
            "code_size": self.code_size,
            "dependencies": {k: {"name": v.name, "version": v.version} for k, v in self.dependencies.items()},
        } #
        return result

@dataclass
class UpgradeConfig:
    """Configuration for the upgrade process"""
    target_language: LanguageType #
    strategy: UpgradeStrategy
    preserve_functionality: bool = True
    update_dependencies: bool = True
    fix_vulnerabilities: bool = True
    improve_performance: bool = True
    add_tests: bool = True #
    modernize_architecture: bool = True
    refactor_code: bool = True
    target_frameworks: List[str] = field(default_factory=list) # Fixed default factory
    excluded_paths: List[str] = field(default_factory=list) # Fixed default factory
    keep_original: bool = True
    max_parallel_processes: int = multiprocessing.cpu_count()
    timeout_seconds: int = 3600  # 1 hour

@dataclass
class UpgradeResult:
    """Results of the upgrade process""" #
    success: bool
    output_path: str
    strategy_used: UpgradeStrategy
    upgraded_files: List[str] = field(default_factory=list) # Fixed default factory
    errors: List[str] = field(default_factory=list) # Fixed default factory
    backup_path: Optional[str] = None
    time_taken_seconds: float = 0.0
    size_difference: int = 0  # Difference in bytes #
    applied_transformations: List[str] = field(default_factory=list) # Fixed default factory
    license_path: Optional[str] = None

# --- LanguageDetector Class ---
# (Keep implementation from system_upgrade_module.txt)
class LanguageDetector:
    """Detects programming languages from file content and extensions""" #
    def __init__(self):
        """Initialize language detector""" #
        self.extension_map = {
            ".py": LanguageType.PYTHON,
            ".js": LanguageType.JAVASCRIPT,
            ".jsx": LanguageType.JAVASCRIPT,
            ".ts": LanguageType.TYPESCRIPT,
            ".tsx": LanguageType.TYPESCRIPT,
            ".java": LanguageType.JAVA,
            ".cs": LanguageType.CSHARP, #
            ".cpp": LanguageType.CPP,
            ".cc": LanguageType.CPP,
            ".cxx": LanguageType.CPP,
            ".c": LanguageType.CPP, # Treat .c as CPP initially, can refine later
            ".h": LanguageType.CPP, # Headers often C++
            ".hpp": LanguageType.CPP,
            ".rb": LanguageType.RUBY, #
            ".php": LanguageType.PHP,
            ".go": LanguageType.GO,
            ".rs": LanguageType.RUST,
            ".swift": LanguageType.SWIFT,
            ".kt": LanguageType.KOTLIN
        }
        self.shebang_patterns = { #
            r"^\s*#!.*python": LanguageType.PYTHON,
            r"^\s*#!.*node": LanguageType.JAVASCRIPT,
            r"^\s*#!.*ruby": LanguageType.RUBY,
            r"^\s*#!.*php": LanguageType.PHP
        }
        self.content_patterns = { #
            # Python: import, from ... import, def, class
            r"^\s*(?:import\s+[\w\.]+)|(?:from\s+[\w\.]+\s+import)|(?:def\s+\w+\s*\(.*\):)|(?:class\s+\w+\s*\(?.*\)?\s*:)" : LanguageType.PYTHON,
            # JS/TS: import, require, function, class, const/let, =>, interface, type
            r"^\s*(?:import\s+.*from\s*['\"])|(?:require\s*\(\s*['\"])|(?:export\s+)|(?:function(?:\s|\*))|(?:class\s+\w+)|(?:(?:const|let|var)\s+\w+)|(?:=>)|(?:(?:interface|type)\s+\w+)" : LanguageType.JAVASCRIPT, # Simplified combined check
            # Java: package, import java, public class
            r"^\s*(?:package\s+[\w\.]+;)|(?:import\s+java\.)|(?:public\s+(?:(?:abstract|final)\s+)?class\s+)" : LanguageType.JAVA,
            # C#: namespace, using System;
            r"^\s*(?:namespace\s+[\w\.]+)|(?:using\s+System)" : LanguageType.CSHARP,
            # C/C++: #include, template<>, std::
            r"^\s*(?:#include\s*[<\"])|(?:template\s*<)|(?:std::)" : LanguageType.CPP,
            # Ruby: require, def, class, module
            r"^\s*(?:require\s+['\"])|(?:def\s+\w+)|(?:class\s+\w+)|(?:module\s+\w+)" : LanguageType.RUBY,
            # PHP: <?php, namespace
            r"<\?php|^\s*namespace\s+[\w\\]+" : LanguageType.PHP,
            # Go: package, import, func
            r"^\s*(?:package\s+\w+)|(?:import\s*\()|(?:func\s+\w+)" : LanguageType.GO, #
            # Rust: use, fn, mod, struct, enum, impl
            r"^\s*(?:use\s+[\w:]+)|(?:fn\s+\w+)|(?:mod\s+\w+)|(?:(?:struct|enum|impl)\s+\w+)" : LanguageType.RUST,
            # Swift: import, class, func, struct, enum, protocol
            r"^\s*(?:import\s+\w+)|(?:(?:class|struct|enum|protocol)\s+\w+)|(?:func\s+\w+)" : LanguageType.SWIFT, #
            # Kotlin: package, import, fun, class
            r"^\s*(?:package\s+[\w\.]+)|(?:import\s+[\w\.]+)|(?:fun\s+\w+)|(?:(?:class|interface|object)\s+\w+)" : LanguageType.KOTLIN
        }

    def detect_language(self, file_path: str, content: Optional[str] = None) -> LanguageType:
        """
        Detect the programming language of a file

        Args:
            file_path: Path to the file #
            content: Optional file content

        Returns:
            Detected language type
        """
        # Try by extension first
        ext = os.path.splitext(file_path)[1].lower()
        if ext in self.extension_map: #
            # Refine C/C++ for .h files based on content if available
            if ext == '.h' and content:
                 if '#include <iostream>' in content or 'std::' in content or 'class ' in content:
                     return LanguageType.CPP
                 else:
                     return LanguageType.C # Default .h to C if no C++ signs
            return self.extension_map[ext]

        # If no content provided, try to read it
        if content is None:
            try:
                # Read only a portion for efficiency
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read(8192) # Read first 8KB #
            except Exception as e:
                logger.warning(f"Could not read {file_path}: {str(e)}")
                # Check if binary before defaulting to UNKNOWN
                try:
                    with open(file_path, 'rb') as f: f.read(10)
                    # If readable as binary but not text, likely binary
                    return LanguageType.UNKNOWN # Or make a guess based on 'file' command if available
                except: pass # Ignore errors reading as binary
                return LanguageType.UNKNOWN #

        # Try by shebang
        first_line = content.split('\n', 1)[0]
        for pattern, lang in self.shebang_patterns.items(): #
            if re.search(pattern, first_line):
                return lang

        # Try by content patterns - check most specific first
        # Score based matching could be more robust
        scores = {lang: 0 for lang in LanguageType}
        for pattern, lang in self.content_patterns.items(): #
            # Weight patterns differently? For now, simple count
            score = len(re.findall(pattern, content, re.MULTILINE))
            if score > 0:
                 # Special handling for JS/TS ambiguity
                 if lang == LanguageType.JAVASCRIPT and ('interface ' in content or ' type ' in content or ': number' in content or ': string' in content):
                      scores[LanguageType.TYPESCRIPT] += score * 1.5 # Boost TS score
                 else:
                     scores[lang] += score

        # Determine best match
        best_lang = LanguageType.UNKNOWN
        max_score = 0
        for lang, score in scores.items():
             if score > max_score:
                 max_score = score
                 best_lang = lang
             # Handle ties? For now, first max wins (dict order dependency - not ideal)

        if max_score > 0:
             return best_lang

        return LanguageType.UNKNOWN #

# --- SystemAnalyzer Class ---
# (Keep implementation from system_upgrade_module.txt)
class SystemAnalyzer:
    """Analyzes a system to gather information needed for upgrading""" #

    def __init__(self):
        """Initialize system analyzer""" #
        self.language_detector = LanguageDetector()
        self.excluded_dirs = {
            ".git", ".svn", ".hg", "node_modules", "__pycache__",
            "venv", "env", ".env", ".venv", "dist", "build", "target", "bin", "obj" # Added common build outputs
        }
        self.excluded_files = { #
            ".DS_Store", "Thumbs.db", ".gitignore", ".dockerignore",
            "package-lock.json", "yarn.lock", "composer.lock", "Gemfile.lock" # Added lock files
        }

    def analyze_system(self, path: str) -> SystemInfo:
        """
        Analyze a system to gather information

        Args:
            path: Path to the system root directory

        Returns: #
            System information
        """
        logger.info(f"Analyzing system at {path}")

        # Initialize system info
        system_info = SystemInfo(
            root_path=path,
            system_type=SystemType.UNKNOWN,
            primary_language=LanguageType.UNKNOWN #
        )

        # Check if path exists
        if not os.path.exists(path):
            raise ValueError(f"Path {path} does not exist")

        # Count languages for later determining primary language
        language_counts = {}

        # Walk through the directory tree #
        total_size = 0
        file_count = 0

        for root, dirs, files in os.walk(path, topdown=True):
            # Skip excluded directories
            dirs[:] = [d for d in dirs if d not in self.excluded_dirs and not d.startswith('.')] # Skip hidden dirs too #

            # Process each file
            for file in files:
                if file in self.excluded_files or file.startswith('.'): # Skip hidden files #
                    continue

                file_path = os.path.join(root, file) #
                relative_path = os.path.relpath(file_path, path)

                try: # Added outer try-except for file-level errors
                    # Skip binary files and large files
                    file_size_bytes = os.path.getsize(file_path)
                    if self._is_binary_file(file_path) or file_size_bytes > 10 * 1024 * 1024:  # 10MB #
                        continue

                    # Read file content
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f: #
                        content = f.read()

                    # Detect language
                    language = self.language_detector.detect_language(file_path, content)

                    # Update language counts #
                    if language != LanguageType.UNKNOWN:
                        language_counts[language] = language_counts.get(language, 0) + 1

                    # Create code file info #
                    code_file = CodeFile(
                        path=relative_path,
                        language=language,
                        content=content #
                    )

                    # Extract imports and other information based on language
                    self._extract_file_info(code_file) #

                    # Add to system info
                    system_info.files[relative_path] = code_file

                    # Update total size #
                    total_size += file_size_bytes # Use calculated size
                    file_count += 1

                    # Check for special files #
                    file_lower = file.lower()
                    if any(name in file_lower for name in ["readme", "license", "dockerfile", "docker-compose"]):
                        # Could add special handling here #
                        pass

                    # Identify potential entry points #
                    if self._is_entry_point(file_path, relative_path, language):
                        system_info.entry_points.append(relative_path) #

                    # Identify configuration files #
                    if self._is_config_file(file_path, relative_path):
                        system_info.config_files.append(relative_path) #

                except Exception as e:
                    logger.warning(f"Error processing file {file_path}: {str(e)}")

        # Set primary language and other languages
        if language_counts:
            primary_language = max(language_counts.items(), key=lambda x: x[1])[0] #
            system_info.primary_language = primary_language
            system_info.other_languages = [lang for lang in language_counts.keys() if lang != primary_language]

        # Determine system type
        system_info.system_type = self._determine_system_type(system_info)

        # Update file count and code size #
        system_info.file_count = file_count
        system_info.code_size = total_size

        # Build dependency graph
        system_info.dependencies_graph = self._build_dependency_graph(system_info)

        # Analyze dependencies
        self._analyze_dependencies(system_info)

        # Identify API endpoints #
        self._identify_api_endpoints(system_info)

        # Check for vulnerabilities
        self._check_vulnerabilities(system_info)

        # Identify database connections
        self._identify_database_connections(system_info)

        logger.info(f"System analysis complete: {system_info.primary_language.name}, {system_info.system_type.name}, {file_count} files")

        return system_info

    def _is_binary_file(self, file_path: str) -> bool: #
        """Check if a file is binary using heuristics"""
        try:
            with open(file_path, 'rb') as f:
                chunk = f.read(1024)
                # Check for null bytes or non-printable ASCII
                return b'\x00' in chunk or any(b > 127 for b in chunk)
        except IOError:
            return True # Treat inaccessible files as potentially binary
        except Exception as e:
            logger.warning(f"Error checking if binary {file_path}: {e}")
            return False # Default to not binary if check fails

    def _extract_file_info(self, code_file: CodeFile) -> None: #
        """Extract imports and other information from file"""
        language = code_file.language
        # content = code_file.content # Already available

        # Use language-specific extractors
        if language == LanguageType.PYTHON:
            self._extract_python_info(code_file) # Use combined method
        elif language == LanguageType.JAVASCRIPT:
            self._extract_javascript_info(code_file) # Use combined method
        elif language == LanguageType.TYPESCRIPT: #
            self._extract_typescript_info(code_file) # Use combined method
        elif language == LanguageType.JAVA:
            self._extract_java_info(code_file) # Use combined method
        # Add elif blocks for other supported languages
        # elif language == LanguageType.CPP: self._extract_cpp_info(code_file)
        # ...

    # --- Combined Language-Specific Info Extractors ---
    def _extract_python_info(self, code_file: CodeFile) -> None:
        """Extract imports, exports (if applicable), functions, classes from Python file"""
        try:
            tree = ast.parse(code_file.content) #
            for node in ast.walk(tree):
                if isinstance(node, ast.Import):
                    for alias in node.names:
                        code_file.imports.append(alias.name) # Store module name
                elif isinstance(node, ast.ImportFrom):
                    module_name = node.module or "" # Handle relative imports (module can be None)
                    for alias in node.names:
                        full_name = f"{module_name}.{alias.name}" if module_name else alias.name
                        code_file.imports.append(full_name) # Store full import path #
                # Basic function/class detection (can be enhanced with params, etc.)
                # elif isinstance(node, ast.FunctionDef): ...
                # elif isinstance(node, ast.ClassDef): ...
        except SyntaxError as e:
            logger.warning(f"Syntax error parsing Python file {code_file.path}: {e}. Falling back to regex.")
            # Fall back to regex for imports if AST fails
            for match in re.finditer(r'^\s*(?:import|from)\s+([\w\.]+)', code_file.content, re.MULTILINE): #
                code_file.imports.append(match.group(1))

    def _extract_javascript_info(self, code_file: CodeFile) -> None:
        """Extract imports, exports, functions, classes from JS file"""
        content = code_file.content
        # Regex for ES6/CommonJS imports/requires (improved slightly)
        import_pattern = r'(?:import(?:["\'\s]*(?:[\w*{}\n\r\s,]+)from\s*)?)(?:["\'](.*?)["\'])|(?:require\s*\(\s*["\'](.*?)["\']\s*\))' #
        for match in re.finditer(import_pattern, content):
            module = match.group(1) or match.group(2)
            if module and module not in code_file.imports: code_file.imports.append(module)
        # Regex for exports
        export_pattern = r'export\s+(?:(?:default(?:\s+async)?\s+function)|(?:(?:const|let|var|function|class)\s+(\w+))|({.*}))' #
        for match in re.finditer(export_pattern, content):
            name = match.group(1) or match.group(2) # Extract named export or group
            if name: code_file.exports.append(name.strip())
        # Add function/class detection if needed

    def _extract_typescript_info(self, code_file: CodeFile) -> None:
        """Extract imports, exports, functions, classes, interfaces, types from TS file"""
        self._extract_javascript_info(code_file) # Inherit JS extraction
        # Add TS specific extraction (interfaces, types) - simplified regex
        # for match in re.finditer(r'^\s*(?:interface|type)\s+(\w+)', code_file.content, re.MULTILINE): ...

    def _extract_java_info(self, code_file: CodeFile) -> None:
        """Extract imports, package, classes, interfaces from Java file"""
        content = code_file.content
        # Package
        package_match = re.search(r'^\s*package\s+([\w\.]+);', content, re.MULTILINE)
        # if package_match: code_file.package = package_match.group(1) # Add 'package' to CodeFile if needed
        # Imports
        for match in re.finditer(r'^\s*import\s+(?:static\s+)?([\w\.]+)(?:\.\*)?;', content, re.MULTILINE): #
             code_file.imports.append(match.group(1))
        # Classes/Interfaces (simplified)
        # for match in re.finditer(r'^\s*(?:public|private|protected)?\s*(?:abstract|final|static)?\s*(class|interface)\s+(\w+)', content, re.MULTILINE): ...

    # ... (Keep remaining SystemAnalyzer methods: _is_entry_point, _is_config_file, _determine_system_type, _build_dependency_graph, _file_provides_import, _analyze_dependencies methods, _identify_api_endpoints methods, _check_vulnerabilities, _identify_database_connections) ...

# --- CodeTransformer Base Class and PythonModernizer ---
# (Keep implementation from system_upgrade_module.txt, including the completed _convert_to_fstrings)
# (Add placeholder _update_imports implementation)
class CodeTransformer(ABC): #
    @abstractmethod
    def can_transform(self, code_file: CodeFile) -> bool: #
        """Check if this transformer can handle the given file"""
        pass
    @abstractmethod
    def transform(self, code_file: CodeFile, system_info: SystemInfo) -> Tuple[str, List[str]]: #
        """
        Transform the code

        Args:
            code_file: Code file to transform
            system_info: System information #

        Returns:
            Tuple of (transformed code, list of applied transformations)
        """
        pass

class PythonModernizer(CodeTransformer):
    """Modernizes Python code"""
    def can_transform(self, code_file: CodeFile) -> bool: #
        """Check if this transformer can handle the given file"""
        return code_file.language == LanguageType.PYTHON
    def transform(self, code_file: CodeFile, system_info: SystemInfo) -> Tuple[str, List[str]]: #
        """Transform Python code to modern standards"""
        content = code_file.content
        transformations = []
        # Add type hints
        content, type_transforms = self._add_type_hints(content) #
        if type_transforms:
            transformations.append("Added type hints")
        # Convert to f-strings
        content, fstring_count = self._convert_to_fstrings(content)
        if fstring_count > 0:
            transformations.append(f"Converted {fstring_count} string formats to f-strings")
        # Use modern Python features
        content, modern_transforms = self._modernize_python_features(content) #
        transformations.extend(modern_transforms)
        # Update imports (placeholder)
        content, import_transforms = self._update_imports(content, system_info)
        transformations.extend(import_transforms)
        return content, transformations

    def _add_type_hints(self, content: str) -> Tuple[str, List[str]]: #
        """Add type hints to Python code"""
        if 'from typing import ' not in content and 'import typing' not in content:
            content = "from typing import List, Dict, Tuple, Optional, Any, Union\n\n" + content
            return content, ["Added basic typing imports"]
        return content, []

    def _convert_to_fstrings(self, content: str) -> Tuple[str, int]: #
        """Convert old-style string formatting to f-strings"""
        # Convert .format() style
        pattern_format = r'([\'"](.*?)(?<!\\)[\'"])\s*\.\s*format\s*\((.*?)\)'
        # Convert % style (handle tuple and dict cases slightly better)
        pattern_percent = r'([\'"](.*?)[\'"])\s*%\s*(\(.*?\)|[\w\.]+)' # Handle tuples or single vars

        count = 0

        def replace_format(match):
            nonlocal count
            orig_call = match.group(0)
            quotes = match.group(1)[0]
            string_template = match.group(2)
            args_str = match.group(3).strip()

            # Heuristic: only convert if args look simple (variables, literals, basic calls)
            # Avoid complex expressions or keyword arguments for this simple regex approach
            if re.fullmatch(r'([\w\.\(\)\[\]\'"]+(\s*,\s*[\w\.\(\)\[\]\'"]+)*)?', args_str):
                try:
                    # Replace {} and {0}, {1} etc.
                    args = [a.strip() for a in args_str.split(',')]
                    fstring_content = string_template
                    implicit_idx = 0
                    explicit_idx_pattern = re.compile(r'\{(\d+)\}')

                    # Replace explicit indices first {0}, {1}, ...
                    def repl_explicit(m):
                        idx = int(m.group(1))
                        return f'{{{args[idx]}}}' if idx < len(args) else m.group(0)
                    fstring_content = explicit_idx_pattern.sub(repl_explicit, fstring_content)

                    # Replace implicit placeholders {}
                    while '{}' in fstring_content and implicit_idx < len(args):
                         fstring_content = fstring_content.replace('{}', f'{{{args[implicit_idx]}}}', 1)
                         implicit_idx += 1

                    # If any {} or {\d+} remain, conversion likely failed/incomplete
                    if '{}' in fstring_content or explicit_idx_pattern.search(fstring_content):
                         logger.debug(f"Skipping complex .format(): {orig_call}")
                         return orig_call

                    # Escape existing braces
                    fstring_content = fstring_content.replace('{', '{{').replace('}', '}}')
                    # Unescape our placeholders
                    fstring_content = fstring_content.replace('{{', '{').replace('}}', '}')

                    new_str = f"f{quotes}{fstring_content}{quotes}"
                    count += 1
                    return new_str
                except (IndexError, Exception) as e: # Catch errors during replacement
                    logger.debug(f"Error converting .format() to f-string for '{orig_call}': {e}")
                    return orig_call
            return orig_call # Skip complex arg lists

        def replace_percent(match):
            nonlocal count
            orig_call = match.group(0)
            quotes = match.group(1)[0]
            string_template = match.group(2)
            args_match = match.group(3).strip()

            # Handle simple cases: single var or tuple of vars/literals
            args = []
            if args_match.startswith('(') and args_match.endswith(')'):
                 args_str = args_match[1:-1]
                 # Very basic parsing of tuple elements
                 args = [a.strip() for a in args_str.split(',') if a.strip()]
            elif re.fullmatch(r'[\w\.]+', args_match): # Single variable
                 args = [args_match]
            else:
                 return orig_call # Skip complex % formatting

            try:
                 # Find % placeholders (%s, %d, %f etc.)
                 placeholders = re.findall(r'%[sdifrx]', string_template)
                 if len(placeholders) == len(args):
                     fstring_content = string_template
                     for i, ph in enumerate(placeholders):
                         # Replace first occurrence of a % placeholder
                         fstring_content = re.sub(r'%[sdifrx]', f'{{{args[i]}}}', fstring_content, count=1)

                     # Escape existing braces
                     fstring_content = fstring_content.replace('{', '{{').replace('}', '}}')
                     # Unescape our placeholders
                     fstring_content = fstring_content.replace('{{', '{').replace('}}', '}')

                     new_str = f"f{quotes}{fstring_content}{quotes}"
                     count += 1
                     return new_str
                 else:
                     logger.debug(f"Argument count mismatch for % format: {orig_call}")
                     return orig_call
            except Exception as e:
                 logger.debug(f"Error converting % to f-string for '{orig_call}': {e}")
                 return orig_call

        content = re.sub(pattern_format, replace_format, content)
        content = re.sub(pattern_percent, replace_percent, content)

        return content, count

    def _modernize_python_features(self, content: str) -> Tuple[str, List[str]]: #
        transformations = []
        # Example: Replace print statements with function calls (Python 2 -> 3)
        if re.search(r'^print\s+[^(\n]', content, re.MULTILINE):
            content = re.sub(r'^print\s+(.+)$', r'print(\1)', content, flags=re.MULTILINE)
            transformations.append("Converted print statements to Python 3 style")
        # Example: Use pathlib for path manipulation (basic - needs context)
        # if 'os.path.join' in content:
        #     # This is complex and requires AST understanding of context
        #     # content = content.replace("os.path.join(a, b)", "(Path(a) / b)") # Oversimplified example
        #     transformations.append("Consider using pathlib for path operations")
        return content, transformations

    def _update_imports(self, content: str, system_info: SystemInfo) -> Tuple[str, List[str]]: #
        # Placeholder: Could use system_info.dependencies to update versions or imports
        # Needs a dependency analysis tool integration (e.g., pip-tools, safety)
        logger.warning("Python import update check not implemented.")
        return content, []

# --- Placeholders for other language modernizers ---
class JavaScriptModernizer(CodeTransformer):
    def can_transform(self, code_file: CodeFile) -> bool:
        return code_file.language == LanguageType.JAVASCRIPT

    def transform(self, code_file: CodeFile, system_info: SystemInfo) -> Tuple[str, List[str]]:
        logger.warning(f"JavaScript modernization not fully implemented for {code_file.path}")
        return code_file.content, ["JavaScript Modernization (Placeholder)"] #

# ... (Add JavaModernizer, CppModernizer etc. placeholders similarly) ...

# --- SystemUpgrader Class ---
# (Keep implementation from previous turn, including the parallel processing wrapper)
class SystemUpgrader:
    """Orchestrates the system upgrade process"""

    def __init__(self):
        self.analyzer = SystemAnalyzer()
        # Register available transformers
        self.transformers: List[CodeTransformer] = [
            PythonModernizer(),
            JavaScriptModernizer(), # Add placeholder
            # Add other transformers here
        ]
        self.logger = logging.getLogger(__name__) #

    def upgrade_system(self, root_path: str, config: UpgradeConfig) -> Dict[str, Any]:
        """Performs the system upgrade based on the configuration"""
        start_time = datetime.datetime.now()
        self.logger.info(f"Starting system upgrade for {root_path} with strategy {config.strategy.name}")

        try:
            # 1. Analyze the system
            system_info = self.analyzer.analyze_system(root_path) #

            # 2. Create backup if requested
            backup_path = None
            if config.keep_original:
                backup_path = self._create_backup(root_path)
                self.logger.info(f"Created backup at {backup_path}")

            # 3. Prepare output directory
            # Using a temporary directory first, then copy to final location if successful
            with tempfile.TemporaryDirectory(prefix="kaleidoscope_upgrade_") as temp_output_dir:
                output_path = temp_output_dir
                self.logger.info(f"Using temporary output directory: {output_path}")

                upgraded_files = []
                errors = []
                all_transformations: Dict[str, List[str]] = {} # File path -> List of transforms

                # 4. Process and Transform files (potentially in parallel)
                files_to_process = list(system_info.files.items())

                # Determine number of processes
                num_processes = min(config.max_parallel_processes, len(files_to_process)) if files_to_process else 1

                if num_processes > 1:
                    self.logger.info(f"Processing {len(files_to_process)} files in parallel with {num_processes} processes...")
                    # Use multiprocessing for parallel transformation
                    # Note: Ensure all args passed to _transform_file_wrapper are picklable
                    # SystemInfo and UpgradeConfig might be large; consider passing only necessary parts
                    # For simplicity, passing full objects assuming they are reasonably picklable
                    pool_args = [(file_path, code_file, system_info, config, self.transformers) # Pass transformers list
                                 for file_path, code_file in files_to_process]
                    try:
                        with multiprocessing.Pool(processes=num_processes) as pool:
                             # Use map_async for potentially better error handling and progress reporting (if needed)
                             async_results = pool.starmap_async(SystemUpgrader._transform_file_wrapper, pool_args)
                             results = async_results.get(timeout=config.timeout_seconds + 60) # Add buffer to timeout
                    except multiprocessing.TimeoutError:
                         errors.append("Parallel processing timed out.")
                         results = [] # Avoid processing partial results on timeout
                    except Exception as pool_e:
                         errors.append(f"Multiprocessing pool error: {pool_e}")
                         results = []
                else:
                     # Process sequentially if only one process or few files
                     self.logger.info(f"Processing {len(files_to_process)} files sequentially...")
                     results = [SystemUpgrader._transform_file_wrapper(fp, cf, system_info, config, self.transformers)
                                for fp, cf in files_to_process]

                # Collect results
                for file_path, new_content, file_transforms, error in results:
                    if error:
                        errors.append(f"Failed to transform {file_path}: {error}") #
                    elif new_content is not None: # Check if transformation happened
                        # Write transformed file to output directory
                        output_file = os.path.join(output_path, file_path)
                        os.makedirs(os.path.dirname(output_file), exist_ok=True)
                        try:
                             with open(output_file, 'w', encoding='utf-8') as f:
                                 f.write(new_content) #
                             upgraded_files.append(file_path)
                             if file_transforms:
                                 all_transformations[file_path] = file_transforms
                        except IOError as write_e:
                             errors.append(f"Failed to write upgraded file {output_file}: {write_e}")
                    # If new_content is None, it means no transformation was applied or needed

                # 5. Update Dependency Files (Example for Python requirements.txt)
                if config.update_dependencies and system_info.primary_language == LanguageType.PYTHON:
                    req_path = os.path.join(root_path, "requirements.txt")
                    output_req_path = os.path.join(output_path, "requirements.txt")
                    if os.path.exists(req_path):
                        try:
                            # Ensure the directory exists before writing
                            os.makedirs(os.path.dirname(output_req_path), exist_ok=True)
                            updated_req = self._update_python_dependencies(req_path)
                            with open(output_req_path, 'w') as f:
                                f.write(updated_req)
                            all_transformations["requirements.txt"] = ["Updated dependencies (basic freeze)"]
                        except Exception as dep_e:
                            errors.append(f"Failed to update dependencies: {dep_e}")
                    # Add similar logic for package.json, pom.xml etc.

                # 6. Copy non-transformed/non-code files
                self._copy_other_files(root_path, output_path, system_info, config.excluded_paths)

                # 7. Finalize Results
                time_taken = (datetime.datetime.now() - start_time).total_seconds()
                size_diff = self._calculate_size_difference(root_path, output_path)

                # Determine final success status
                final_success = len(errors) == 0

                # Move from temp to final location only if successful or if errors are acceptable
                # For now, always move and report errors. Could add a flag to abort on error.
                final_output_path = f"{root_path}_upgraded_{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}"
                try:
                    shutil.move(output_path, final_output_path) # Move from temp to final location
                except Exception as move_e:
                    errors.append(f"Failed to move temporary output to final destination: {move_e}")
                    final_output_path = output_path # Keep temp path if move fails
                    final_success = False # Mark as failed if move fails

                upgrade_result = UpgradeResult(
                    success=final_success,
                    output_path=final_output_path, # Use the final path
                    strategy_used=config.strategy, #
                    upgraded_files=upgraded_files,
                    errors=errors,
                    backup_path=backup_path,
                    time_taken_seconds=time_taken,
                    size_difference=size_diff, #
                    applied_transformations=[f"{f}: {', '.join(t)}" for f, t in all_transformations.items()] # Flatten dict
                ) #

                self.logger.info(f"Upgrade process finished in {time_taken:.2f}s. Success: {upgrade_result.success}")
                return upgrade_result.__dict__ # Return as dict

        except Exception as e:
            self.logger.error(f"System upgrade failed: {str(e)}", exc_info=True)
            # Ensure backup_path is defined even on early failure
            backup_path = None
            if config.keep_original and 'backup_path' not in locals():
                 # Attempt backup even if analysis failed, might be useful
                 backup_path = self._create_backup(root_path)

            return UpgradeResult(
                success=False,
                output_path=root_path, # No new output path created
                strategy_used=config.strategy,
                errors=[f"Critical error during upgrade: {str(e)}"],
                backup_path=backup_path,
                time_taken_seconds=(datetime.datetime.now() - start_time).total_seconds()
            ).__dict__

    # Static method for multiprocessing safety (doesn't rely on self)
    @staticmethod
    def _transform_file_wrapper(file_path: str, code_file: CodeFile, system_info: SystemInfo, config: UpgradeConfig, transformers: List[CodeTransformer]):
        """Wrapper function for parallel file transformation"""
        # Note: This now runs in a separate process. Logging here might go to a different place or get lost.
        # Consider using multiprocessing.log_to_stderr() or a queue for logging from workers.
        try:
            transformer = next((t for t in transformers if t.can_transform(code_file)), None)
            if transformer and config.refactor_code: # Check if refactoring is enabled #
                # Add logic here to potentially use LLM service for refactoring/modernization
                # Since LLMClient instance isn't easily passed, this might require:
                # 1. Making LLMClient usable across processes (difficult with some backends)
                # 2. Performing LLM tasks *after* the parallel processing phase in the main process
                # 3. Using an external LLM API endpoint accessible from workers
                new_content, file_transforms = transformer.transform(code_file, system_info)
                return file_path, new_content, file_transforms, None
            else:
                # If no transformer or refactoring disabled, return original content but indicate no change
                return file_path, None, [], None # Return None for content if no change applied
        except Exception as e:
            # Log error if possible, but primarily return it
            # logger.error(f"Error transforming {file_path} in worker: {e}") # This logger might not work as expected
            return file_path, None, [], f"Worker error: {str(e)}" # Return original content and error

    def _create_backup(self, root_path: str) -> Optional[str]: # Added Optional return type
        """Creates a backup of the original system"""
        backup_dir = f"{root_path}_backup_{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}" # More descriptive name #
        try:
            # Ignore common patterns like .git, node_modules etc. during backup
            ignore_patterns = shutil.ignore_patterns('.git*', 'node_modules*', '*__pycache__*', '*.pyc', '*.log', '*.tmp', 'venv*', '.venv*')
            shutil.copytree(root_path, backup_dir, ignore=ignore_patterns)
            self.logger.info(f"Backup created at {backup_dir}")
            return backup_dir
        except Exception as e:
            self.logger.error(f"Failed to create backup: {e}")
            return None #

    def _calculate_size_difference(self, original_path: str, new_path: str) -> int: #
        """Calculates the size difference between original and new paths"""
        try:
            orig_size = sum(p.stat().st_size for p in Path(original_path).rglob('*') if p.is_file())
            new_size = sum(p.stat().st_size for p in Path(new_path).rglob('*') if p.is_file())
            return new_size - orig_size
        except Exception as e:
            self.logger.error(f"Could not calculate size difference: {e}")
            return 0 #

    def _copy_other_files(self, src_dir, dest_dir, system_info: SystemInfo, excluded_paths: List[str]): #
         """Copies non-code files and explicitly excluded files to the destination."""
         abs_excluded_paths = {os.path.abspath(os.path.join(src_dir, p)) for p in excluded_paths}
         abs_processed_files = {os.path.abspath(os.path.join(src_dir, p)) for p in system_info.files.keys()}

         for root, dirs, files in os.walk(src_dir, topdown=True):
             abs_root = os.path.abspath(root)
             # Filter excluded directories
             dirs[:] = [d for d in dirs if d not in self.excluded_dirs and not d.startswith('.') and os.path.join(abs_root, d) not in abs_excluded_paths]

             for file in files:
                 src_file_path = os.path.abspath(os.path.join(root, file))

                 # Skip if it's an analyzed code file or explicitly excluded or in excluded dirs
                 if src_file_path in abs_processed_files or \
                    src_file_path in abs_excluded_paths or \
                    any(src_file_path.startswith(ex_dir) for ex_dir in abs_excluded_paths if os.path.isdir(ex_dir)) or \
                    file in self.excluded_files or file.startswith('.'):
                     continue

                 rel_path = os.path.relpath(src_file_path, src_dir)
                 dest_file_path = os.path.join(dest_dir, rel_path)
                 try:
                     os.makedirs(os.path.dirname(dest_file_path), exist_ok=True)
                     shutil.copy2(src_file_path, dest_file_path) # Use copy2 to preserve metadata
                 except Exception as e:
                     self.logger.warning(f"Could not copy file {src_file_path} to {dest_file_path}: {e}")

    def _update_python_dependencies(self, requirements_path: str) -> str: #
        """Updates Python dependencies using pip freeze (basic)."""
        # Placeholder: Requires pip-compile or similar tool for proper updates
        # Example using pip freeze (less accurate for updates)
        self.logger.warning("Dependency update is basic (pip freeze). Use pip-tools for better results.")
        try:
            # Determine Python executable (prefer venv if exists)
            venv_python = Path.cwd() / "venv" / ("Scripts" if sys.platform == 'win32' else "bin") / "python"
            python_exe = str(venv_python) if venv_python.exists() else sys.executable

            result = subprocess.run([python_exe, "-m", "pip", "freeze"], capture_output=True, text=True, check=True)
            return result.stdout
        except Exception as e:
            self.logger.error(f"Failed to update Python dependencies via pip freeze: {e}")
            # Fallback to original content
            try:
                with open(requirements_path, 'r') as f: return f.read()
            except IOError:
                return "# Failed to read original requirements.txt" #

# --- Main Execution Block (Example) ---
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Kaleidoscope AI - System Upgrade Module")
    parser.add_argument("path", help="Root path of the software system to upgrade")
    parser.add_argument("--target", default="python", choices=[lang.name.lower() for lang in LanguageType if lang != LanguageType.UNKNOWN], help="Target language")
    parser.add_argument("--strategy", default="in_place", choices=[strat.name.lower() for strat in UpgradeStrategy], help="Upgrade strategy")
    # Add more args based on UpgradeConfig
    parser.add_argument("--no-backup", action="store_false", dest="keep_original", help="Do not create a backup")
    parser.add_argument("--update-deps", action="store_true", default=True, help="Update dependencies") # Default True
    parser.add_argument("--fix-vulns", action="store_true", default=True, help="Fix vulnerabilities") # Default True
    parser.add_argument("--processes", type=int, default=multiprocessing.cpu_count(), help="Max parallel processes")

    args = parser.parse_args()

    # Ensure path exists
    if not os.path.isdir(args.path):
        logger.critical(f"Input path is not a valid directory: {args.path}")
        sys.exit(1)

    # Create config
    upgrade_config = UpgradeConfig(
        target_language=LanguageType[args.target.upper()],
        strategy=UpgradeStrategy[args.strategy.upper()],
        keep_original=args.keep_original,
        update_dependencies=args.update_deps,
        fix_vulnerabilities=args.fix_vulns,
        max_parallel_processes=args.processes
        # Add other config args here
    )

    # Run upgrade
    upgrader = SystemUpgrader()
    result_dict = upgrader.upgrade_system(args.path, upgrade_config) #

    # Print results
    print("\n--- Upgrade Result ---")
    print(json.dumps(result_dict, indent=2)) # Print result dict

    if result_dict["success"]:
         print("\nUpgrade completed successfully!")
         print(f"Upgraded system saved to: {result_dict['output_path']}")
         if result_dict['backup_path']:
             print(f"Original system backed up to: {result_dict['backup_path']}")
    else:
         print("\nUpgrade completed with errors.")
         if result_dict['output_path'] != args.path: # Check if temp dir was moved
             print(f"Partially upgraded system (with errors) saved to: {result_dict['output_path']}")
         if result_dict['backup_path']:
             print(f"Original system backed up to: {result_dict['backup_path']}")
         print("Errors encountered:")
         for err in result_dict['errors']: print(f" - {err}")
Summary of Changes & Next Steps:

Completed PythonModernizer: Finished the _convert_to_fstrings and added placeholders for _modernize_python_features and _update_imports.
Added Placeholder Transformers: Included basic structures for JavaScriptModernizer and potentially others.
Implemented SystemUpgrader: Added the main class to orchestrate analysis, backup, parallel transformation (using multiprocessing.Pool), dependency updates (basic pip freeze), copying other files, and result generation. Includes error handling and temporary directory usage.
Fixed Dataclass Defaults: Corrected field.default_factory.list to field(default_factory=list) and similar errors.
Refined SystemAnalyzer: Improved binary file detection, added more comprehensive language detection patterns, refined info extraction helpers (_extract_python_info, etc.).
Enhanced Main Block: Added more command-line arguments corresponding to UpgradeConfig and improved result printing.
Remaining Work & Next Steps:

Implement Placeholders:
Finish PythonModernizer._modernize_python_features and _update_imports (potentially integrating dependency check tools).
Implement actual logic for JavaScriptModernizer, JavaModernizer, etc.
Implement vulnerability checking beyond basic regex (_check_vulnerabilities).
Implement test generation (_generate_tests in KaleidoscopePlatform) and architecture modernization logic if desired.
Integrate unravel_ai_core_engine.py:
Use FileAnalyzer within SystemAnalyzer for more robust analysis.
Use Decompiler outputs as input for transformation tasks.
Use SpecGenerator as an optional task.
Refine kaleidoscope_main.py:
Integrate the SystemUpgrader workflow properly.
Schedule analysis and upgrade steps using OptimizedTaskScheduler.
Pass results between components (e.g., analysis results to upgrader).
Define Missing Core Classes: Implement CoreLaws, CapabilityNode, NodeManager, MemoryGraph, etc.
Testing: Add comprehensive unit and integration tests.
Please reply with 'k' if you'd like me to proceed with integrating the unravel_ai_core_engine.py components and refining the main orchestrator.


Sources and related content
system

TXT
systemfixe...additions3

TXT
systemadditions

TXT
error_handler

TXT
error_handler (1)

TXT





k



Show thinking
Okay, continuing with the implementation of src/system_upgrade_module.py.

5. src/system_upgrade_module.py (Completed)
(Continuing from the previous response, completing PythonModernizer, adding placeholders, and implementing SystemUpgrader)

Python

# src/system_upgrade_module.py (Continued and Completed)
import os
import sys
import re
import ast
import json
import shutil
import tempfile
import subprocess
import importlib
import logging
import zipfile
import tarfile
import uuid
import hashlib
import datetime
import docker # Note: Imported but not used in this specific code - potentially for future containerized upgrades
import multiprocessing
from pathlib import Path
from typing import Dict, List, Set, Tuple, Any, Optional, Union, Callable
from dataclasses import dataclass, field
from enum import Enum, auto
from abc import ABC, abstractmethod
import networkx as nx

# Setup logging using the configuration utility
# Assuming src is in the Python path or adjust relative import
try:
    from src.utils.logging_config import get_logger
except ImportError:
    # Fallback if running standalone or structure differs
    logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(name)s: %(message)s')
    logger = logging.getLogger(__name__)
    def get_logger(name): return logging.getLogger(name)

logger = get_logger(__name__)

# --- Enums ---
class LanguageType(Enum):
    """Supported programming languages"""
    PYTHON = auto()
    JAVASCRIPT = auto()
    TYPESCRIPT = auto()
    JAVA = auto()
    CSHARP = auto()
    CPP = auto()
    RUBY = auto()
    PHP = auto()
    GO = auto()
    RUST = auto()
    SWIFT = auto()
    KOTLIN = auto()
    UNKNOWN = auto()

class SystemType(Enum):
    """Types of systems to upgrade"""
    WEB_APP = auto()
    DESKTOP_APP = auto()
    MOBILE_APP = auto()
    API = auto()
    CLI = auto()
    LIBRARY = auto()
    FRAMEWORK = auto()
    DATABASE = auto()
    UNKNOWN = auto()

class UpgradeStrategy(Enum):
    """Strategies for system upgrades"""
    IN_PLACE = auto()
    INCREMENTAL = auto()
    FULL_REWRITE = auto()
    LANGUAGE_MIGRATION = auto()
    WRAPPER = auto()

# --- Dataclasses ---
@dataclass
class DependencyInfo:
    """Information about a dependency"""
    name: str
    version: str
    current_version: Optional[str] = None
    latest_version: Optional[str] = None
    is_vulnerable: bool = False
    vulnerability_details: Optional[str] = None
    is_outdated: bool = False
    upgrade_path: Optional[str] = None
    is_deprecated: bool = False
    alternatives: List[str] = field(default_factory=list)

@dataclass
class CodeFile:
    """Information about a code file"""
    path: str
    language: LanguageType
    content: str
    ast: Optional[Any] = None # AST node if parsed
    imports: List[str] = field(default_factory=list)
    exports: List[str] = field(default_factory=list)
    dependencies: List[DependencyInfo] = field(default_factory=list) # Dependencies inferred from this file
    vulnerabilities: List[str] = field(default_factory=list) # Vulnerabilities found in this file
    outdated_patterns: List[str] = field(default_factory=list) # Outdated patterns found
    complexity_score: float = 0.0 # e.g., Cyclomatic complexity
    is_test: bool = False # Whether this is a test file

@dataclass
class SystemInfo:
    """Information about the system to upgrade"""
    root_path: str
    system_type: SystemType
    primary_language: LanguageType
    other_languages: List[LanguageType] = field(default_factory=list)
    files: Dict[str, CodeFile] = field(default_factory=dict) # relative_path -> CodeFile
    dependencies: Dict[str, DependencyInfo] = field(default_factory=dict) # package_name -> DependencyInfo
    entry_points: List[str] = field(default_factory=list) # List of relative paths
    config_files: List[str] = field(default_factory=list) # List of relative paths
    database_info: Dict[str, Any] = field(default_factory=dict) # Info about DB connections/types
    api_endpoints: List[str] = field(default_factory=list) # Detected API endpoints
    vulnerabilities: List[str] = field(default_factory=list) # Project-level vulnerabilities
    dependencies_graph: Optional[nx.DiGraph] = None # File dependency graph
    file_count: int = 0
    code_size: int = 0  # In bytes

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary"""
        result = {
            "root_path": self.root_path,
            "system_type": self.system_type.name,
            "primary_language": self.primary_language.name,
            "other_languages": [lang.name for lang in self.other_languages],
            "entry_points": self.entry_points,
            "config_files": self.config_files,
            "database_info": self.database_info,
            "api_endpoints": self.api_endpoints,
            "vulnerabilities": self.vulnerabilities,
            "file_count": self.file_count,
            "code_size": self.code_size,
            # Convert dependencies dict values to basic dicts
            "dependencies": {k: v.__dict__ for k, v in self.dependencies.items()},
            # Files dict might be too large; maybe exclude content or AST?
            "files_summary": {path: {"language": cf.language.name, "size": len(cf.content)} for path, cf in self.files.items()}
        }
        return result

@dataclass
class UpgradeConfig:
    """Configuration for the upgrade process"""
    target_language: LanguageType
    strategy: UpgradeStrategy
    preserve_functionality: bool = True
    update_dependencies: bool = True
    fix_vulnerabilities: bool = True
    improve_performance: bool = True
    add_tests: bool = True
    modernize_architecture: bool = True
    refactor_code: bool = True
    target_frameworks: List[str] = field(default_factory=list)
    excluded_paths: List[str] = field(default_factory=list) # Relative paths from root
    keep_original: bool = True
    max_parallel_processes: int = multiprocessing.cpu_count()
    timeout_seconds: int = 3600  # 1 hour

@dataclass
class UpgradeResult:
    """Results of the upgrade process"""
    success: bool
    output_path: str
    strategy_used: UpgradeStrategy
    upgraded_files: List[str] = field(default_factory=list) # List of relative paths
    errors: List[str] = field(default_factory=list)
    backup_path: Optional[str] = None
    time_taken_seconds: float = 0.0
    size_difference: int = 0  # Difference in bytes
    applied_transformations: List[str] = field(default_factory=list) # Summary of transformations
    license_path: Optional[str] = None

# --- LanguageDetector Class ---
# (Implementation from previous turn)
class LanguageDetector:
    """Detects programming languages from file content and extensions"""
    def __init__(self):
        """Initialize language detector"""
        self.extension_map = {
            ".py": LanguageType.PYTHON, ".pyw": LanguageType.PYTHON,
            ".js": LanguageType.JAVASCRIPT, ".jsx": LanguageType.JAVASCRIPT, ".mjs": LanguageType.JAVASCRIPT,
            ".ts": LanguageType.TYPESCRIPT, ".tsx": LanguageType.TYPESCRIPT,
            ".java": LanguageType.JAVA,
            ".cs": LanguageType.CSHARP,
            ".cpp": LanguageType.CPP, ".cc": LanguageType.CPP, ".cxx": LanguageType.CPP,
            ".c": LanguageType.CPP, # Treat C as CPP for simplicity here, can refine
            ".h": LanguageType.CPP, ".hpp": LanguageType.CPP, # Treat headers as C++
            ".rb": LanguageType.RUBY,
            ".php": LanguageType.PHP,
            ".go": LanguageType.GO,
            ".rs": LanguageType.RUST,
            ".swift": LanguageType.SWIFT,
            ".kt": LanguageType.KOTLIN, ".kts": LanguageType.KOTLIN
        }
        self.shebang_patterns = {
            r"^\s*#!.*python": LanguageType.PYTHON,
            r"^\s*#!.*node": LanguageType.JAVASCRIPT,
            r"^\s*#!.*ruby": LanguageType.RUBY,
            r"^\s*#!.*php": LanguageType.PHP
        }
        self.content_patterns = {
            r"^\s*(?:import\s+[\w\.]+)|(?:from\s+[\w\.]+\s+import)|(?:def\s+\w+\s*\(.*\):)|(?:class\s+\w+\s*\(?.*\)?\s*:)" : LanguageType.PYTHON,
            r"^\s*(?:import\s+.*from\s*['\"])|(?:require\s*\(\s*['\"])|(?:export\s+)|(?:function(?:\s|\*))|(?:class\s+\w+)|(?:(?:const|let|var)\s+\w+)|(?:=>)|(?:(?:interface|type)\s+\w+)" : LanguageType.JAVASCRIPT, # Combined JS/TS check
            r"^\s*(?:package\s+[\w\.]+;)|(?:import\s+java\.)|(?:public\s+(?:(?:abstract|final)\s+)?class\s+)" : LanguageType.JAVA,
            r"^\s*(?:namespace\s+[\w\.]+)|(?:using\s+System)" : LanguageType.CSHARP,
            r"^\s*(?:#include\s*[<\"])|(?:template\s*<)|(?:std::)" : LanguageType.CPP, # C/C++
            r"^\s*(?:require\s+['\"])|(?:def\s+\w+)|(?:class\s+\w+)|(?:module\s+\w+)" : LanguageType.RUBY,
            r"<\?php|^\s*namespace\s+[\w\\]+" : LanguageType.PHP,
            r"^\s*(?:package\s+\w+)|(?:import\s*\()|(?:func\s+\w+)" : LanguageType.GO,
            r"^\s*(?:use\s+[\w:]+)|(?:fn\s+\w+)|(?:mod\s+\w+)|(?:(?:struct|enum|impl)\s+\w+)" : LanguageType.RUST,
            r"^\s*(?:import\s+\w+)|(?:(?:class|struct|enum|protocol)\s+\w+)|(?:func\s+\w+)" : LanguageType.SWIFT,
            r"^\s*(?:package\s+[\w\.]+)|(?:import\s+[\w\.]+)|(?:fun\s+\w+)|(?:(?:class|interface|object)\s+\w+)" : LanguageType.KOTLIN
        }

    def detect_language(self, file_path: str, content: Optional[str] = None) -> LanguageType:
        """Detect the programming language of a file"""
        # Try by extension first
        ext = os.path.splitext(file_path)[1].lower()
        if ext in self.extension_map:
            # Refine C/C++ for .h files based on content if available
            if ext == '.h' and content:
                 if '#include <iostream>' in content or 'std::' in content or 'class ' in content:
                     return LanguageType.CPP
                 else:
                     # Could check for specific C library includes if needed
                     return LanguageType.C # Default .h to C if no C++ signs
            return self.extension_map[ext]

        # If no content provided, try to read it
        if content is None:
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read(8192) # Read first 8KB
            except Exception as e:
                logger.debug(f"Could not read {file_path} for content detection: {str(e)}")
                return LanguageType.UNKNOWN # Default to unknown if unreadable

        # Try by shebang
        first_line = content.split('\n', 1)[0]
        for pattern, lang in self.shebang_patterns.items():
            if re.search(pattern, first_line):
                return lang

        # Try by content patterns - check most specific first
        scores = {lang: 0 for lang in LanguageType}
        for pattern, lang in self.content_patterns.items():
            score = len(re.findall(pattern, content, re.MULTILINE))
            if score > 0:
                 # Handle JS/TS ambiguity
                 if lang == LanguageType.JAVASCRIPT and ('interface ' in content or ' type ' in content or ': number' in content or ': string' in content):
                      scores[LanguageType.TYPESCRIPT] += score * 1.5 # Boost TS score
                 else:
                     scores[lang] += score

        # Determine best match
        best_lang = LanguageType.UNKNOWN
        max_score = 0
        sorted_scores = sorted(scores.items(), key=lambda item: item[1], reverse=True)
        if sorted_scores and sorted_scores[0][1] > 0:
             best_lang = sorted_scores[0][0]
             # Handle potential ties or low confidence if needed
             # if len(sorted_scores) > 1 and sorted_scores[0][1] == sorted_scores[1][1]:
             #     logger.debug(f"Ambiguous language detection for {file_path}, defaulting to {best_lang.name}")

        return best_lang

# --- SystemAnalyzer Class ---
# (Keep implementation from system_upgrade_module.txt)
class SystemAnalyzer:
    """Analyzes a system to gather information needed for upgrading"""
    def __init__(self):
        """Initialize system analyzer"""
        self.language_detector = LanguageDetector()
        self.excluded_dirs = {
            ".git", ".svn", ".hg", "node_modules", "__pycache__",
            "venv", "env", ".env", ".venv", "dist", "build", "target", "bin", "obj"
        }
        self.excluded_files = {
            ".DS_Store", "Thumbs.db", ".gitignore", ".dockerignore",
            "package-lock.json", "yarn.lock", "composer.lock", "Gemfile.lock"
        }
        self.logger = get_logger(__name__) # Use configured logger

    def analyze_system(self, path: str) -> SystemInfo:
        """Analyze a system to gather information"""
        self.logger.info(f"Analyzing system at {path}")
        system_info = SystemInfo(root_path=path, system_type=SystemType.UNKNOWN, primary_language=LanguageType.UNKNOWN)
        if not os.path.isdir(path): # Check if directory exists
            raise ValueError(f"Path {path} does not exist or is not a directory")

        language_counts: Dict[LanguageType, int] = {}
        total_size = 0
        file_count = 0

        for root, dirs, files in os.walk(path, topdown=True):
            # Skip excluded directories
            dirs[:] = [d for d in dirs if d not in self.excluded_dirs and not d.startswith('.')]

            for file in files:
                if file in self.excluded_files or file.startswith('.'):
                    continue

                file_path = os.path.join(root, file)
                relative_path = os.path.relpath(file_path, path)

                try:
                    file_size_bytes = os.path.getsize(file_path)
                    # Skip binary files and large files
                    if file_size_bytes > 20 * 1024 * 1024: # Increased limit to 20MB
                        self.logger.debug(f"Skipping large file: {relative_path} ({file_size_bytes} bytes)")
                        continue
                    if self._is_binary_file(file_path):
                        self.logger.debug(f"Skipping binary file: {relative_path}")
                        continue

                    # Read file content
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read()

                    language = self.language_detector.detect_language(file_path, content)
                    if language != LanguageType.UNKNOWN:
                        language_counts[language] = language_counts.get(language, 0) + 1

                    code_file = CodeFile(path=relative_path, language=language, content=content)
                    self._extract_file_info(code_file) # Extract imports etc.
                    system_info.files[relative_path] = code_file

                    total_size += file_size_bytes
                    file_count += 1

                    # Check special files
                    if any(name in file.lower() for name in ["readme", "license"]):
                        if "license" in file.lower(): system_info.license_path = relative_path
                    if self._is_entry_point(file_path, relative_path, language):
                        system_info.entry_points.append(relative_path)
                    if self._is_config_file(file_path, relative_path):
                        system_info.config_files.append(relative_path)

                except OSError as oe: # Handle OS errors like permission denied
                     self.logger.warning(f"OS error processing file {file_path}: {oe}")
                except Exception as e:
                    self.logger.warning(f"Error processing file {file_path}: {e}")

        # Finalize system info
        if language_counts:
            system_info.primary_language = max(language_counts, key=language_counts.get)
            system_info.other_languages = [lang for lang, count in language_counts.items() if lang != system_info.primary_language]
        system_info.system_type = self._determine_system_type(system_info)
        system_info.file_count = file_count
        system_info.code_size = total_size
        try: # Wrap graph building in try-except
             system_info.dependencies_graph = self._build_dependency_graph(system_info)
        except Exception as graph_e:
             self.logger.error(f"Failed to build dependency graph: {graph_e}")
        self._analyze_dependencies(system_info)
        self._identify_api_endpoints(system_info)
        self._check_vulnerabilities(system_info) # Basic check
        self._identify_database_connections(system_info)

        self.logger.info(f"System analysis complete: Primary={system_info.primary_language.name}, Type={system_info.system_type.name}, Files={file_count}, Size={total_size} bytes")
        return system_info

    # ... (Keep other SystemAnalyzer helper methods: _is_binary_file, _extract_file_info methods, _is_entry_point, etc.) ...
    def _is_binary_file(self, file_path: str) -> bool: #
        """Check if a file is binary using heuristics"""
        try:
            with open(file_path, 'rb') as f:
                chunk = f.read(1024)
                # Check for null bytes or non-printable ASCII
                return b'\x00' in chunk or any(b > 127 for b in chunk)
        except IOError:
            return True # Treat inaccessible files as potentially binary
        except Exception as e:
            self.logger.warning(f"Error checking if binary {file_path}: {e}")
            return False # Default to not binary if check fails

    def _extract_file_info(self, code_file: CodeFile) -> None: #
        """Extract imports and other information from file"""
        language = code_file.language
        # Use language-specific extractors
        if language == LanguageType.PYTHON:
            self._extract_python_info(code_file) # Use combined method
        elif language == LanguageType.JAVASCRIPT:
            self._extract_javascript_info(code_file) # Use combined method
        elif language == LanguageType.TYPESCRIPT: #
            self._extract_typescript_info(code_file) # Use combined method
        elif language == LanguageType.JAVA:
            self._extract_java_info(code_file) # Use combined method
        # Add elif blocks for other supported languages
        # elif language == LanguageType.CPP: self._extract_cpp_info(code_file)
        # ...

    # --- Combined Language-Specific Info Extractors ---
    def _extract_python_info(self, code_file: CodeFile) -> None:
        """Extract imports, exports (if applicable), functions, classes from Python file"""
        try:
            tree = ast.parse(code_file.content) #
            code_file.ast = tree # Store AST if needed later
            for node in ast.walk(tree):
                if isinstance(node, ast.Import):
                    for alias in node.names:
                        # Store module name, handle potential aliases if needed
                        if alias.name not in code_file.imports: code_file.imports.append(alias.name)
                elif isinstance(node, ast.ImportFrom):
                    module_name = node.module or "" # Handle relative imports (module can be None)
                    for alias in node.names:
                        # Combine module and name unless it's a relative import from same dir
                        full_name = f"{module_name}.{alias.name}" if module_name and node.level == 0 else alias.name
                        if full_name not in code_file.imports: code_file.imports.append(full_name) #
        except SyntaxError as e:
            self.logger.warning(f"Syntax error parsing Python file {code_file.path}: {e}. Falling back to regex for imports.")
            # Fall back to regex for imports if AST fails
            for match in re.finditer(r'^\s*(?:import|from)\s+([\w\.]+)', code_file.content, re.MULTILINE): #
                 if match.group(1) not in code_file.imports: code_file.imports.append(match.group(1))
        # Add regex or AST logic for functions/classes if needed

    def _extract_javascript_info(self, code_file: CodeFile) -> None:
        """Extract imports, exports, functions, classes from JS file"""
        content = code_file.content
        # Regex for ES6/CommonJS imports/requires (improved slightly)
        import_pattern = r'(?:import(?:["\'\s]*(?:[\w*{}\n\r\s,]+)from\s*)?)(?:["\']([@\w\-./]+)["\'])|(?:require\s*\(\s*["\']([@\w\-./]+)["\']\s*\))' # Allow scoped packages and paths
        for match in re.finditer(import_pattern, content):
            module = match.group(1) or match.group(2)
            if module and module not in code_file.imports: code_file.imports.append(module)
        # Regex for exports
        export_pattern = r'export\s+(?:(?:default(?:\s+async)?\s+function)|(?:(?:const|let|var|function|class)\s+(\w+))|({.*}))' #
        for match in re.finditer(export_pattern, content):
            # Extract named export or group (simplify group extraction)
            name = match.group(1) or (match.group(2).strip() if match.group(2) else None)
            if name and name not in code_file.exports: code_file.exports.append(name)
        # Add function/class detection if needed

    def _extract_typescript_info(self, code_file: CodeFile) -> None:
        """Extract imports, exports, functions, classes, interfaces, types from TS file"""
        self._extract_javascript_info(code_file) # Inherit JS extraction
        # Add TS specific extraction (interfaces, types) - simplified regex
        ts_specific_pattern = r'^\s*(?:interface|type|enum)\s+(\w+)'
        for match in re.finditer(ts_specific_pattern, code_file.content, re.MULTILINE):
            # Could store these separately if needed
            pass

    def _extract_java_info(self, code_file: CodeFile) -> None:
        """Extract imports, package, classes, interfaces from Java file"""
        content = code_file.content
        # Package
        package_match = re.search(r'^\s*package\s+([\w\.]+);', content, re.MULTILINE)
        # if package_match: code_file.package = package_match.group(1) # Add 'package' to CodeFile if needed
        # Imports
        for match in re.finditer(r'^\s*import\s+(?:static\s+)?([\w\.]+)(?:\.\*)?;', content, re.MULTILINE): #
             if match.group(1) not in code_file.imports: code_file.imports.append(match.group(1))
        # Classes/Interfaces (simplified)
        # Add regex for class/interface detection if needed

    def _is_entry_point(self, file_path: str, relative_path: str, language: LanguageType) -> bool: #
        """Identify if a file is an entry point"""
        file_name = os.path.basename(file_path).lower()
        # Common entry point patterns
        if language == LanguageType.PYTHON: #
            # Check common names or presence of if __name__ == '__main__':
            if file_name in ["main.py", "app.py", "manage.py", "run.py", "__main__.py"]: return True
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    return "if __name__ == '__main__':" in f.read() or 'if __name__ == "__main__":' in f.read()
            except Exception: return False
        elif language in [LanguageType.JAVASCRIPT, LanguageType.TYPESCRIPT]:
            # Check common names or package.json main field (requires package.json analysis elsewhere)
            return file_name in ["index.js", "main.js", "app.js", "server.js", "index.ts", "main.ts", "app.ts", "server.ts"]
        elif language == LanguageType.JAVA:
            # Check for public static void main(String... args)
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    return "public static void main(" in f.read()
            except Exception: return False #
        elif language == LanguageType.CSHARP:
            # Check for static void Main(string[] args) or top-level statements in Program.cs
            if "program.cs" in file_path.lower(): return True # Common convention
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    return "static void Main(" in f.read()
            except Exception: return False
        # Add checks for other languages (Go main package/func, C/C++ main func)
        elif language == LanguageType.GO:
             try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                    return 'package main' in content and 'func main()' in content
             except Exception: return False
        elif language == LanguageType.CPP: # Includes C
             try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    # Basic check for main function definition
                    return re.search(r'\bmain\s*\([^)]*\)\s*{', f.read()) is not None
             except Exception: return False

        return False

    def _is_config_file(self, file_path: str, relative_path: str) -> bool: #
        """Identify if a file is a configuration file"""
        file_name = os.path.basename(file_path).lower()
        ext = os.path.splitext(file_name)[1].lower()
        config_extensions = {'.json', '.yaml', '.yml', '.ini', '.toml', '.xml', '.env', '.conf', '.cfg'}
        config_filenames = {
            'config', 'settings', 'properties', 'dockerfile', 'docker-compose.yml', 'docker-compose.yaml',
            'package.json', 'composer.json', 'pyproject.toml', 'requirements.txt', 'gemfile',
            '.env', '.env.example', '.env.local', '.npmrc', '.yarnrc', '.gitconfig', '.editorconfig'
        } #
        return ext in config_extensions or file_name in config_filenames or file_name.startswith('.')

    def _determine_system_type(self, system_info: SystemInfo) -> SystemType: #
        """Determine the type of system based on files and languages"""
        files = system_info.files.keys() # Use relative paths
        top_level_items = {item.split(os.sep, 1)[0] for item in files}

        # Web app indicators
        web_indicators_files = {"index.html", "package.json", "webpack.config.js", "angular.json", "vue.config.js", "next.config.js"}
        web_indicators_dirs = {"views", "templates", "public", "static", "assets", "pages", "components", "routes"} # More specific web dirs
        web_indicators_langs = {LanguageType.JAVASCRIPT, LanguageType.TYPESCRIPT, LanguageType.HTML, LanguageType.CSS} # Add HTML/CSS if detected

        # API indicators
        api_indicators_dirs = {"controllers", "routes", "api", "endpoints", "graphql"}
        api_indicators_files = {"swagger.json", "openapi.yaml"}
        api_indicators_content = {"@RequestMapping", "@Path", "@RestController", "express()", "FastAPI"} # Framework specific keywords

        # Desktop app indicators
        desktop_indicators_libs = {"electron", "qt", "gtk", "wxwidgets", "javafx", "wpf", "winforms"} # Library names

        # Scores
        web_score = 0
        api_score = 0
        desktop_score = 0
        library_score = 0

        # Score based on files/dirs
        web_score += sum(1 for item in top_level_items if item in web_indicators_dirs or item in web_indicators_files)
        api_score += sum(1 for item in top_level_items if item in api_indicators_dirs or item in api_indicators_files)
        # Add scoring for desktop indicators if needed

        # Check for library indicators
        if "setup.py" in top_level_items or ("package.json" in top_level_items and "main" in system_info.files.get("package.json", CodeFile(path="",language=LanguageType.UNKNOWN,content="{}")).content):
            library_score += 5 # Strong indicator

        # Score based on primary language
        if system_info.primary_language in {LanguageType.PYTHON, LanguageType.RUBY, LanguageType.PHP, LanguageType.JAVASCRIPT, LanguageType.TYPESCRIPT}:
            web_score += 2
            api_score += 2
        elif system_info.primary_language in {LanguageType.JAVA, LanguageType.CSHARP}:
            web_score += 1
            api_score += 1
            desktop_score += 2
        elif system_info.primary_language == LanguageType.SWIFT or system_info.primary_language == LanguageType.KOTLIN:
             # Could indicate mobile, but let's keep it simple for now
             pass

        # Score based on content analysis (simplified)
        for code_file in system_info.files.values():
             content_lower = code_file.content.lower()
             if any(ind in content_lower for ind in api_indicators_content): api_score += 0.5
             if any(lib in content_lower for lib in desktop_indicators_libs): desktop_score += 1

        # Determine type
        scores = {
            SystemType.WEB_APP: web_score,
            SystemType.API: api_score,
            SystemType.DESKTOP_APP: desktop_score,
            SystemType.LIBRARY: library_score
        }
        # Add CLI check? e.g., presence of argparse/click in Python, main func in Go/C etc.

        if not scores or max(scores.values()) < 2: # Require some threshold
            return SystemType.UNKNOWN
        else:
            return max(scores, key=scores.get)

    def _build_dependency_graph(self, system_info: SystemInfo) -> Optional[nx.DiGraph]: # Return Optional
        """Build a dependency graph of files"""
        try:
            G = nx.DiGraph() #
            file_map = {info.path: info for info in system_info.files.values()}

            # Add all files as nodes
            for file_path in system_info.files:
                G.add_node(file_path, language=system_info.files[file_path].language.name) # Add language as attribute

            # Add edges based on imports
            for file_path, code_file in system_info.files.items():
                for imported in code_file.imports:
                    # Try to find the corresponding file (simple relative path check)
                    potential_target_path = self._resolve_import_path(file_path, imported, code_file.language, system_info.files.keys())
                    if potential_target_path and potential_target_path in G:
                        G.add_edge(file_path, potential_target_path) #

            return G
        except Exception as e:
            self.logger.error(f"Failed to build dependency graph: {e}")
            return None

    def _resolve_import_path(self, current_file: str, import_str: str, language: LanguageType, all_files: Set[str]) -> Optional[str]:
        """Attempt to resolve an import string to a relative file path within the project."""
        # This is complex and language-specific. Provide a basic placeholder.
        # Needs proper handling of relative paths, aliases, package structures etc.

        if language == LanguageType.PYTHON:
            # Example: from .utils import helper -> utils/helper.py or utils/helper/__init__.py
            parts = import_str.split('.')
            base_dir = os.path.dirname(current_file)

            # Simplistic check: look for matching .py file or directory/__init__.py
            potential_path_py = os.path.normpath(os.path.join(base_dir, *parts)) + ".py"
            potential_path_init = os.path.normpath(os.path.join(base_dir, *parts, "__init__.py"))

            if potential_path_py in all_files: return potential_path_py
            if potential_path_init in all_files: return potential_path_init

        elif language in [LanguageType.JAVASCRIPT, LanguageType.TYPESCRIPT]:
             # Example: import { func } from './utils/helper'; -> utils/helper.js or .ts
             if import_str.startswith('.'): # Relative import
                 base_dir = os.path.dirname(current_file)
                 potential_path = os.path.normpath(os.path.join(base_dir, import_str))
                 # Check for .js, .ts, .jsx, .tsx extensions
                 for ext in ['.js', '.ts', '.jsx', '.tsx']:
                     if potential_path + ext in all_files: return potential_path + ext
                 # Check for index file in directory
                 for ext in ['index.js', 'index.ts']:
                      if os.path.join(potential_path, ext) in all_files: return os.path.join(potential_path, ext)

        # Add logic for other languages...

        return None # Unable to resolve

    def _file_provides_import(self, code_file: CodeFile, import_name: str) -> bool: #
        """Check if a file provides the given import"""
        # This is too simplistic, rely on _resolve_import_path instead
        # Keeping for backward compatibility with analyze_system if needed, but ideally remove
        file_basename = os.path.splitext(os.path.basename(code_file.path))[0]
        # Handle index files (e.g., index.js maps to directory import)
        if file_basename == 'index':
             dir_name = os.path.basename(os.path.dirname(code_file.path))
             return dir_name == import_name or import_name.endswith(f"/{dir_name}")
        # Handle __init__.py (maps to package import)
        if file_basename == '__init__':
             pkg_name = os.path.basename(os.path.dirname(code_file.path))
             return pkg_name == import_name or import_name.endswith(f".{pkg_name}")

        return file_basename == import_name or import_name.endswith(f".{file_basename}")

    # --- Dependency Analysis Methods ---
    # (Keep implementations from system_upgrade_module.txt) ...
    def _analyze_dependencies(self, system_info: SystemInfo) -> None: #
        """Analyze external dependencies"""
        # Extract dependencies from common dependency files
        for file_path in system_info.config_files:
            full_path = os.path.join(system_info.root_path, file_path) #

            if "requirements.txt" in file_path:
                self._extract_python_dependencies(system_info, full_path)
            elif "package.json" in file_path:
                self._extract_npm_dependencies(system_info, full_path)
            elif "composer.json" in file_path:
                 self._extract_composer_dependencies(system_info, full_path) #
            elif "gemfile" in file_path.lower():
                self._extract_ruby_dependencies(system_info, full_path)
            elif "build.gradle" in file_path or "pom.xml" in file_path:
                self._extract_java_dependencies(system_info, full_path)
        # Add logic here to check dependency versions, vulnerabilities (using external tools/APIs)

    def _extract_python_dependencies(self, system_info: SystemInfo, file_path: str) -> None: #
        """Extract Python dependencies from requirements.txt"""
        try:
            with open(file_path, 'r') as f:
                for line in f:
                    line = line.strip()
                    if not line or line.startswith('#') or line.startswith('-'): # Ignore comments and options
                         continue #

                    # Parse dependency (improved regex to handle specifiers)
                    match = re.match(r'^\s*([\w\-\.]+)\s*([=<>]?=?\s*[\w\.\*\-]+)?', line)
                    if match:
                        name = match.group(1).strip()
                        version = match.group(2).strip() if match.group(2) else "" #
                        # Add to dependencies only if not already present with potentially better version info
                        if name not in system_info.dependencies or not system_info.dependencies[name].version:
                            system_info.dependencies[name] = DependencyInfo(name=name, version=version) #
        except Exception as e:
            self.logger.warning(f"Error parsing {file_path}: {str(e)}")

    def _extract_npm_dependencies(self, system_info: SystemInfo, file_path: str) -> None: #
        """Extract NPM dependencies from package.json"""
        try:
            with open(file_path, 'r') as f:
                data = json.load(f) #

                # Process dependencies
                for dep_type in ['dependencies', 'devDependencies', 'peerDependencies']: # Include peerDeps
                    if dep_type in data and isinstance(data[dep_type], dict): # Check if dict
                        for name, version in data[dep_type].items(): #
                            # Add to dependencies
                            system_info.dependencies[name] = DependencyInfo(name=name, version=version)
        except Exception as e:
            self.logger.warning(f"Error parsing {file_path}: {str(e)}") #

    def _extract_composer_dependencies(self, system_info: SystemInfo, file_path: str) -> None: #
        """Extract PHP dependencies from composer.json"""
        try:
            with open(file_path, 'r') as f:
                data = json.load(f)

                # Process dependencies #
                for dep_type in ['require', 'require-dev']:
                    if dep_type in data and isinstance(data[dep_type], dict): # Check if dict
                        for name, version in data[dep_type].items():
                            # Add to dependencies #
                            system_info.dependencies[name] = DependencyInfo(name=name, version=version)
        except Exception as e:
            self.logger.warning(f"Error parsing {file_path}: {str(e)}") #

    def _extract_ruby_dependencies(self, system_info: SystemInfo, file_path: str) -> None: #
        """Extract Ruby dependencies from Gemfile"""
        try:
            with open(file_path, 'r') as f: #
                for line in f:
                    line = line.strip()
                    if not line or line.startswith('#'):
                        continue #

                    # Match gem declarations (improved regex)
                    match = re.match(r'gem\s+[\'"]([^\'"]+)[\'"](?:,\s*[\'"]([~<>=]*\s*[\d\.\w]+)[\'"])?', line) # Capture version specifier
                    if match:
                        name = match.group(1) #
                        version = match.group(2) or "" #

                        # Add to dependencies #
                        system_info.dependencies[name] = DependencyInfo(name=name, version=version) #
        except Exception as e:
            self.logger.warning(f"Error parsing {file_path}: {str(e)}")

    def _extract_java_dependencies(self, system_info: SystemInfo, file_path: str) -> None: #
        """Extract Java dependencies from build.gradle or pom.xml"""
        try:
            if file_path.endswith("build.gradle") or file_path.endswith(".gradle.kts"): # Add kts support
                with open(file_path, 'r') as f:
                    content = f.read()
                    # Extract dependencies from build.gradle using regex (improved for different configs)
                    # Covers common configurations like implementation, api, compileOnly, runtimeOnly etc.
                    pattern = r'(?:implementation|api|compileOnly|runtimeOnly|testImplementation)\s*\(?[\'"]([^:]+):([^:]+):([^\'"]+)[\'"]\)?'
                    for match in re.finditer(pattern, content): #
                        group = match.group(1)
                        name = match.group(2)
                        version = match.group(3)
                        full_name = f"{group}:{name}" #
                        system_info.dependencies[full_name] = DependencyInfo(name=full_name, version=version) #
            elif file_path.endswith("pom.xml"):
                # Simple regex-based extraction for POMs (consider using XML parser for robustness)
                with open(file_path, 'r') as f:
                    content = f.read() #
                    # Extract dependencies from pom.xml (improved to handle optional version better)
                    dependencies = re.findall(r'<dependency>\s*<groupId>([^<]+)</groupId>\s*<artifactId>([^<]+)</artifactId>(?:\s*<version>([^<]+)</version>)?', content) # Make version optional
                    for group, artifact, version in dependencies: #
                        full_name = f"{group}:{artifact}"
                        system_info.dependencies[full_name] = DependencyInfo(name=full_name, version=version or "") # Use empty string if version missing
        except Exception as e:
            self.logger.warning(f"Error parsing {file_path}: {str(e)}")

    # --- API Endpoint Identification Methods ---
    # (Keep implementations from system_upgrade_module.txt) ...
    def _identify_api_endpoints(self, system_info: SystemInfo) -> None: #
        """Identify API endpoints"""
        # Only process if it's an API or web app
        if system_info.system_type not in [SystemType.API, SystemType.WEB_APP]:
            return
        endpoints = []
        for file_path, code_file in system_info.files.items():
            # Check based on language #
            if code_file.language == LanguageType.PYTHON:
                self._extract_python_endpoints(code_file, endpoints)
            elif code_file.language in [LanguageType.JAVASCRIPT, LanguageType.TYPESCRIPT]:
                self._extract_js_endpoints(code_file, endpoints)
            elif code_file.language == LanguageType.JAVA:
                self._extract_java_endpoints(code_file, endpoints)
            # Add more languages as needed #
        system_info.api_endpoints = sorted(list(set(endpoints))) # Ensure unique and sorted

    def _extract_python_endpoints(self, code_file: CodeFile, endpoints: List[str]) -> None: #
        """Extract API endpoints from Python file"""
        content = code_file.content
        # Flask/Blueprint endpoints
        for match in re.finditer(r'@(?:app|blueprint)\.route\([\'"]([^\'"]+)[\'"]', content): #
            endpoints.append(match.group(1))
        # Django URLs
        for match in re.finditer(r'path\([\'"]([^\'"]+)[\'"]', content):
             # Basic path, might need joining with include prefixes
            endpoints.append(match.group(1))
        # FastAPI endpoints
        for match in re.finditer(r'@(?:app|router)\.(?:get|post|put|delete|patch|options|head|trace)\([\'"]([^\'"]+)[\'"]', content): # Added more methods
            endpoints.append(match.group(1)) #

    def _extract_js_endpoints(self, code_file: CodeFile, endpoints: List[str]) -> None: #
        """Extract API endpoints from JavaScript/TypeScript file"""
        content = code_file.content
        # Express.js endpoints (more specific)
        for method in ['get', 'post', 'put', 'delete', 'patch', 'use']: # Added 'use' for middleware routes
            # Capture route string literal, potentially with variables :id
            for match in re.finditer(rf'\.(?:{method})\s*\(\s*[\'"`]([^\'"`]+)[\'"`]', content): # Handle backticks
                endpoints.append(match.group(1)) #
        # Generic route definitions (less reliable)
        # for match in re.finditer(r'route\s*\(\s*[\'"]([^\'"]+)[\'"]', content):
        #     endpoints.append(match.group(1))

    def _extract_java_endpoints(self, code_file: CodeFile, endpoints: List[str]) -> None: #
        """Extract API endpoints from Java file"""
        content = code_file.content
        # Spring endpoints (including specific methods)
        spring_pattern = r'@(?:RequestMapping|GetMapping|PostMapping|PutMapping|DeleteMapping|PatchMapping)\(\s*(?:value\s*=\s*)?[\'"]([^\'"]+)[\'"]' # Handle value=
        for match in re.finditer(spring_pattern, content): #
            endpoints.append(match.group(1))
        # JAX-RS endpoints
        for match in re.finditer(r'@Path\(\s*[\'"]([^\'"]+)[\'"]', content): #
            endpoints.append(match.group(1))

    # --- Vulnerability and Database Methods ---
    # (Keep implementations from system_upgrade_module.txt) ...
    def _check_vulnerabilities(self, system_info: SystemInfo) -> None: #
        """Check for known vulnerabilities"""
        vulnerabilities = []
        # In a real implementation, this would use a security database or API
        # For now, we'll look for some common vulnerability patterns
        security_patterns = { #
            # SQL Injection (very basic patterns)
            r'(?:select|insert|update|delete)\s+.*\s+where\s+.*\+\s*["\']': "Potential SQL Injection (String Concatenation)",
            r'execute(?:Query|Update)?\s*\(.*?\+\s*["\']': "Potential SQL Injection (execute with concat)",
            # XSS
            r'\.innerHTML\s*=': "Potential XSS via innerHTML", #
            r'document\.write\s*\(': "Potential XSS via document.write",
            # Hardcoded credentials
            r'(?:password|secret|passwd|pwd|api_?key|token)\s*[=:]\s*["\'][^"\']+["\']': "Hardcoded credentials", # Broader check
            # Command injection
            r'(?:Runtime\.getRuntime\(\)\.exec|ProcessBuilder|os\.system|subprocess\.call|exec|spawn)\s*\(".*?\+\s*': "Potential Command Injection (String Concatenation)",
            # Insecure file operations (placeholder - needs context)
            r'eval\s*\(': "Insecure eval() usage", #
            r'pickle\.load': "Potential Deserialization Issue (pickle.load)",
            r'strcpy\(|strcat\(|sprintf\(.*%s': "Potential Buffer Overflow (C/C++)" # Basic C/C++ check
        } #

        for file_path, code_file in system_info.files.items():
            content = code_file.content
            for pattern, issue in security_patterns.items():
                try: # Add try-except around regex
                     if re.search(pattern, content, re.IGNORECASE | re.MULTILINE): # Add MULTILINE flag
                         # Add more context if possible (line number)
                         line_num = -1
                         try:
                             # Find first match line number (can be slow)
                             for i, line in enumerate(content.splitlines()):
                                 if re.search(pattern, line, re.IGNORECASE):
                                     line_num = i + 1
                                     break
                         except Exception: pass # Ignore line finding errors

                         vulnerability = f"{issue} in {file_path}" + (f" near line {line_num}" if line_num > 0 else "")
                         if vulnerability not in vulnerabilities: vulnerabilities.append(vulnerability) # Avoid duplicates per file
                         if issue not in code_file.vulnerabilities: code_file.vulnerabilities.append(issue) # Add to file vulnerabilities
                except Exception as re_e:
                     self.logger.debug(f"Regex error checking pattern '{pattern}' in {file_path}: {re_e}")

        # Also check for outdated dependencies with known vulnerabilities
        # TODO: Integrate with dependency checker tool (e.g., safety, npm audit)

        system_info.vulnerabilities = vulnerabilities #

    def _identify_database_connections(self, system_info: SystemInfo) -> None: #
        """Identify database connections"""
        db_info: Dict[str, Any] = {"detected_types": set(), "config_files": [], "code_files": [], "connection_params": {}}
        # Database patterns to look for
        db_patterns = { #
            "mysql": r'mysql|mysqli|pdo_mysql|mysql\.connector|pymysql',
            "postgres": r'postgres|pg_connect|pdo_pgsql|psycopg2', # Added psycopg2
            "sqlite": r'sqlite|pdo_sqlite|sqlite3', # Added sqlite3
            "mongodb": r'mongodb|mongo_connect|pymongo', # Added pymongo
            "oracle": r'oracle|oci_connect|cx_Oracle', # Added cx_Oracle
            "sqlserver": r'sqlserver|mssql|pdo_sqlsrv|pyodbc', # Added pyodbc
            "redis": r'redis', # Added Redis
            "mariadb": r'mariadb' # Added MariaDB
        }

        # Check configuration files first
        for file_path in system_info.config_files: #
            full_path = os.path.join(system_info.root_path, file_path)
            try:
                with open(full_path, 'r', encoding='utf-8', errors='ignore') as f: # Ensure utf-8
                    content = f.read().lower()
                    # Look for connection strings/params
                    found_in_config = False
                    for db_type, pattern in db_patterns.items(): #
                        if re.search(pattern, content, re.IGNORECASE):
                            db_info["detected_types"].add(db_type)
                            found_in_config = True
                    if found_in_config: db_info["config_files"].append(file_path)

                    # Look for connection parameters (simplified)
                    for param in ["host", "port", "database", "dbname", "user", "username", "pass", "password", "uri", "url", "dsn"]: # Added common param names
                        # Improved regex to find key-value pairs
                        matches = re.finditer(rf'["\']?{param}["\']?\s*[=:]\s*["\']?([^"\'\s,]+)["\']?', content, re.IGNORECASE) #
                        for match in matches: #
                             value = match.group(1)
                             # Avoid capturing placeholders like ${DB_HOST}
                             if not re.match(r'^\$\{.+\}$', value):
                                 # Store first found value for simplicity
                                 if param not in db_info["connection_params"]:
                                     db_info["connection_params"][param] = value #
            except Exception as e:
                self.logger.warning(f"Error checking database info in {file_path}: {str(e)}")

        # Also check code files
        for file_path, code_file in system_info.files.items(): #
            # Skip large files or non-code files for performance
            if code_file.language == LanguageType.UNKNOWN or system_info.files[file_path].code_size > 500 * 1024: # Skip files > 500KB
                 continue
            content = code_file.content.lower()
            # Look for database imports and connection code
            found_in_code = False
            for db_type, pattern in db_patterns.items(): #
                if re.search(pattern, content, re.IGNORECASE):
                    db_info["detected_types"].add(db_type)
                    found_in_code = True
            if found_in_code: db_info["code_files"].append(file_path)

        # Convert set to list for JSON serialization
        db_info["detected_types"] = sorted(list(db_info["detected_types"]))
        # Remove duplicates from file lists
        db_info["config_files"] = sorted(list(set(db_info["config_files"])))
        db_info["code_files"] = sorted(list(set(db_info["code_files"])))

        system_info.database_info = db_info #

# --- CodeTransformer Base Class and PythonModernizer ---
# (Keep implementations from previous turn [cite: 1680-1688, 1705-1714]) ...
class CodeTransformer(ABC):
    """Base class for code transformers"""
    @abstractmethod
    def can_transform(self, code_file: CodeFile) -> bool: #
        """Check if this transformer can handle the given file"""
        pass
    @abstractmethod
    def transform(self, code_file: CodeFile, system_info: SystemInfo) -> Tuple[str, List[str]]: #
        """
        Transform the code

        Args:
            code_file: Code file to transform
            system_info: System information #

        Returns:
            Tuple of (transformed code, list of applied transformations)
        """
        pass

class PythonModernizer(CodeTransformer):
    """Modernizes Python code"""
    def can_transform(self, code_file: CodeFile) -> bool: #
        """Check if this transformer can handle the given file"""
        return code_file.language == LanguageType.PYTHON
    def transform(self, code_file: CodeFile, system_info: SystemInfo) -> Tuple[str, List[str]]: #
        """Transform Python code to modern standards"""
        content = code_file.content
        transformations = []
        # Add type hints
        content, type_transforms = self._add_type_hints(content) #
        if type_transforms:
            transformations.extend(type_transforms) # Use extend
        # Convert to f-strings
        content, fstring_count = self._convert_to_fstrings(content)
        if fstring_count > 0:
            transformations.append(f"Converted {fstring_count} string formats to f-strings")
        # Use modern Python features
        content, modern_transforms = self._modernize_python_features(content) #
        transformations.extend(modern_transforms)
        # Update imports (placeholder)
        content, import_transforms = self._update_imports(content, system_info)
        transformations.extend(import_transforms)
        return content, transformations

    def _add_type_hints(self, content: str) -> Tuple[str, List[str]]: #
        """Add basic typing import if missing."""
        # Placeholder implementation - real type hinting needs AST analysis or LLM
        if 'from typing import ' not in content and 'import typing' not in content:
            # Add import at the top, checking for existing docstring/shebang
            lines = content.splitlines()
            insert_pos = 0
            if lines and lines[0].startswith("#!"): insert_pos = 1
            if lines[insert_pos:] and lines[insert_pos].strip().startswith('"""'): insert_pos += 1 # Skip module docstring
            # Check for existing imports before adding
            has_imports = any(line.strip().startswith('import ') or line.strip().startswith('from ') for line in lines)
            if not has_imports or lines[insert_pos].strip() == "": # Add after shebang/docstring, before code or after blank line
                 lines.insert(insert_pos, "\nfrom typing import List, Dict, Tuple, Optional, Any, Union\n") # Add blank lines
                 content = "\n".join(lines)
                 return content, ["Added basic typing imports"]
        return content, []

    def _convert_to_fstrings(self, content: str) -> Tuple[str, int]: #
        """Convert old-style string formatting to f-strings"""
        # Convert .format() style
        pattern_format = r'([\'"](.*?)(?<!\\)[\'"])\s*\.\s*format\s*\((.*?)\)'
        # Convert % style (handle tuple and dict cases slightly better)
        pattern_percent = r'([\'"](.*?)[\'"])\s*%\s*(\(.*?\)|%(?!\()[\w\.\(\)\[\]\'"]+|[\w\.\(\)\[\]\'"]+)' # Handle tuples, %vars, single vars

        count = 0

        def replace_format(match):
            nonlocal count
            orig_call = match.group(0)
            quotes = match.group(1)[0] # Detect quote type ' or " #
            string_template = match.group(2)
            args_str = match.group(3).strip() #

            # Heuristic: only convert if args look simple (variables, literals, basic calls)
            # Avoid complex expressions or keyword arguments for this simple regex approach
            if re.fullmatch(r'([\w\.\(\)\[\]\'"\+\-\*\/]+(\s*,\s*[\w\.\(\)\[\]\'"\+\-\*\/]+)*)?', args_str): # Allow basic ops
                try:
                    # Replace {} placeholders with {arg_name} based on order
                    args = [a.strip() for a in args_str.split(',')] # Simple split
                    fstring_content = string_template
                    implicit_idx = 0
                    explicit_idx_pattern = re.compile(r'\{(\d+)\}')

                    # Replace explicit indices first {0}, {1}, ...
                    def repl_explicit(m):
                        idx = int(m.group(1))
                        return f'{{{args[idx]}}}' if idx < len(args) else m.group(0)
                    fstring_content = explicit_idx_pattern.sub(repl_explicit, fstring_content)

                    # Replace implicit placeholders {}
                    while '{}' in fstring_content and implicit_idx < len(args):
                         fstring_content = fstring_content.replace('{}', f'{{{args[implicit_idx]}}}', 1)
                         implicit_idx += 1

                    # If any {} or {\d+} remain, conversion likely failed/incomplete
                    if '{}' in fstring_content or explicit_idx_pattern.search(fstring_content):
                         logger.debug(f"Skipping complex .format(): {orig_call}")
                         return orig_call

                    # Escape existing braces
                    fstring_content = fstring_content.replace('{', '{{').replace('}', '}}')
                    # Unescape our placeholders
                    fstring_content = fstring_content.replace('{{', '{').replace('}}', '}')

                    new_str = f"f{quotes}{fstring_content}{quotes}"
                    count += 1
                    return new_str
                except (IndexError, Exception) as e: # Catch errors during replacement #
                    logger.debug(f"Error converting .format() to f-string for '{orig_call}': {e}")
                    return orig_call
            return orig_call # Skip complex arg lists

        def replace_percent(match):
            nonlocal count
            orig_call = match.group(0)
            quotes = match.group(1)[0]
            string_template = match.group(2)
            args_match = match.group(3).strip()

            # Handle simple cases: single var or tuple of vars/literals
            args = []
            is_tuple = False
            if args_match.startswith('(') and args_match.endswith(')'):
                 args_str = args_match[1:-1]
                 # Very basic parsing of tuple elements - might fail on complex items
                 args = [a.strip() for a in args_str.split(',') if a.strip()]
                 is_tuple = True
            elif re.fullmatch(r'[\w\.\(\)\[\]\'"]+', args_match): # Single variable/literal/simple call
                 args = [args_match]
            else:
                 logger.debug(f"Skipping complex % format args: {orig_call}")
                 return orig_call # Skip complex % formatting args

            try:
                 # Find % placeholders (%s, %d, %f etc.)
                 placeholders = re.findall(r'%[sdifrx%]', string_template) # Include %%
                 num_real_placeholders = len([p for p in placeholders if p != '%%'])

                 if num_real_placeholders == len(args):
                     fstring_content = string_template
                     arg_idx = 0
                     # Iterate through string to replace placeholders in order
                     last_pos = 0
                     result_parts = []
                     for m in re.finditer(r'%[sdifrx%]', fstring_content):
                          placeholder = m.group(0)
                          result_parts.append(fstring_content[last_pos:m.start()])
                          if placeholder == '%%':
                               result_parts.append('%') # Keep literal %
                          else:
                               result_parts.append(f'{{{args[arg_idx]}}}')
                               arg_idx += 1
                          last_pos = m.end()
                     result_parts.append(fstring_content[last_pos:])
                     fstring_content = "".join(result_parts)

                     # Escape existing braces
                     fstring_content = fstring_content.replace('{', '{{').replace('}', '}}')
                     # Unescape our placeholders
                     fstring_content = fstring_content.replace('{{', '{').replace('}}', '}')

                     new_str = f"f{quotes}{fstring_content}{quotes}"
                     count += 1
                     return new_str
                 else:
                     logger.debug(f"Argument count mismatch for % format ({len(args)} args vs {num_real_placeholders} placeholders): {orig_call}")
                     return orig_call
            except Exception as e:
                 logger.debug(f"Error converting % to f-string for '{orig_call}': {e}")
                 return orig_call

        # Apply replacements carefully
        try:
             content_new_format = re.sub(pattern_format, replace_format, content)
             content_final = re.sub(pattern_percent, replace_percent, content_new_format)
             return content_final, count
        except Exception as sub_e:
             logger.error(f"Error during regex substitution for f-strings: {sub_e}")
             return content, 0 # Return original content on error

    def _modernize_python_features(self, content: str) -> Tuple[str, List[str]]: #
        transformations = []
        # Example: Replace print statements with function calls (Python 2 -> 3)
        if re.search(r'^(\s*)print\s+([^(\s].*)$', content, re.MULTILINE):
            # Careful substitution to handle indentation and avoid changing print() calls
            content = re.sub(r'^(\s*)print\s+([^(\s].*)$', r'\1print(\2)', content, flags=re.MULTILINE)
            transformations.append("Converted print statements to Python 3 style")
        # Example: Suggest using pathlib (requires manual change)
        if 'os.path.join' in content or 'os.path.exists' in content:
             if "Suggested using pathlib" not in transformations: # Add only once
                 transformations.append("Suggested using pathlib for path operations")
        # Example: Suggest using context managers for files
        if re.search(r'\bf\s*=\s*open\(', content) and not re.search(r'with open\(.* as \w+:', content):
             if "Suggested using 'with open()'" not in transformations:
                 transformations.append("Suggested using 'with open()' for file handling")

        return content, transformations

    def _update_imports(self, content: str, system_info: SystemInfo) -> Tuple[str, List[str]]: #
        # Placeholder: This requires external tools like `pip-tools` or `safety`
        # to check for outdated/vulnerable dependencies based on system_info.dependencies
        # and then potentially modifying requirements.txt or pyproject.toml.
        # For now, just returns original content.
        logger.warning("Python import update check not implemented in PythonModernizer.")
        return content, []

# --- Placeholders for other language modernizers ---
class JavaScriptModernizer(CodeTransformer):
    def can_transform(self, code_file: CodeFile) -> bool:
        return code_file.language in [LanguageType.JAVASCRIPT, LanguageType.TYPESCRIPT] # Handle both

    def transform(self, code_file: CodeFile, system_info: SystemInfo) -> Tuple[str, List[str]]:
        logger.warning(f"JavaScript/TypeScript modernization not fully implemented for {code_file.path}")
        content = code_file.content
        transformations = []
        # Example: Basic 'var' to 'let'/'const' (Needs AST for scope correctness)
        # num_vars = len(re.findall(r'\bvar\b', content))
        # if num_vars > 0:
        #    content = re.sub(r'\bvar\b', 'let', content) # Naive replacement
        #    transformations.append(f"Replaced {num_vars} 'var' keywords with 'let' (scope check needed)")
        return content, ["JavaScript/TS Modernization (Placeholder)"] #

# --- SystemUpgrader Class ---
# (Implementation from previous turn)
class SystemUpgrader:
    """Orchestrates the system upgrade process"""

    def __init__(self):
        self.analyzer = SystemAnalyzer()
        # Register available transformers
        self.transformers: List[CodeTransformer] = [
            PythonModernizer(),
            JavaScriptModernizer(), # Add placeholder
            # Add other transformers here
        ]
        self.logger = get_logger(__name__) #

    def upgrade_system(self, root_path: str, config: UpgradeConfig) -> Dict[str, Any]:
        """Performs the system upgrade based on the configuration"""
        start_time = datetime.datetime.now()
        self.logger.info(f"Starting system upgrade for {root_path} with strategy {config.strategy.name}")
        result_obj = UpgradeResult(
             success=False, output_path=root_path, strategy_used=config.strategy
        ) # Initialize result object

        try:
            # 1. Analyze the system
            system_info = self.analyzer.analyze_system(root_path) #

            # 2. Create backup if requested
            backup_path = None
            if config.keep_original:
                backup_path = self._create_backup(root_path)
                if backup_path:
                    self.logger.info(f"Created backup at {backup_path}")
                    result_obj.backup_path = backup_path
                else:
                    # If backup fails, should we abort? Or just warn? Warn for now.
                    self.logger.warning("Backup creation failed, proceeding without backup.")


            # 3. Prepare output directory (use temp dir)
            with tempfile.TemporaryDirectory(prefix="kaleidoscope_upgrade_") as temp_output_dir:
                output_path = temp_output_dir
                self.logger.info(f"Using temporary output directory: {output_path}")

                upgraded_files = []
                errors = []
                all_transformations: Dict[str, List[str]] = {} # File path -> List of transforms

                # 4. Process and Transform files (potentially in parallel)
                files_to_process = list(system_info.files.items())
                num_processes = min(config.max_parallel_processes, len(files_to_process)) if files_to_process else 1
                results = []

                if num_processes > 1 and len(files_to_process) > num_processes: # Only use pool if beneficial
                    self.logger.info(f"Processing {len(files_to_process)} files in parallel with {num_processes} processes...")
                    # Prepare args carefully - avoid passing large objects if possible
                    # Transformers list is small, config is small, system_info might be large
                    # Consider passing only relevant parts of system_info if it becomes a bottleneck
                    pool_args = [(file_path, code_file, None, config, self.transformers) # Pass None for system_info in worker?
                                 for file_path, code_file in files_to_process]
                    try:
                        # Using ProcessPoolExecutor might be safer for CPU-bound tasks & isolation
                        with multiprocessing.Pool(processes=num_processes) as pool:
                             async_results = pool.starmap_async(SystemUpgrader._transform_file_wrapper, pool_args)
                             results = async_results.get(timeout=config.timeout_seconds + 60) # Add buffer to timeout
                    except multiprocessing.TimeoutError:
                         errors.append("Parallel processing timed out.")
                    except Exception as pool_e:
                         errors.append(f"Multiprocessing pool error: {pool_e}")
                else:
                     # Process sequentially
                     self.logger.info(f"Processing {len(files_to_process)} files sequentially...")
                     results = [SystemUpgrader._transform_file_wrapper(fp, cf, system_info, config, self.transformers)
                                for fp, cf in files_to_process]

                # Collect results
                for file_path, new_content, file_transforms, error in results:
                    if error:
                        errors.append(f"Transform {file_path}: {error}") # Include file path in error
                    elif new_content is not None: # Transformation applied
                        output_file = os.path.join(output_path, file_path)
                        try:
                            os.makedirs(os.path.dirname(output_file), exist_ok=True)
                            with open(output_file, 'w', encoding='utf-8') as f:
                                f.write(new_content) #
                            upgraded_files.append(file_path)
                            if file_transforms:
                                all_transformations[file_path] = file_transforms
                        except IOError as write_e:
                             errors.append(f"Write failed {output_file}: {write_e}")
                    # else: new_content is None -> no transformation applied, file will be copied later

                # 5. Update Dependency Files
                if config.update_dependencies:
                     # Update logic for different languages based on system_info
                     if system_info.primary_language == LanguageType.PYTHON:
                         req_path = os.path.join(root_path, "requirements.txt")
                         output_req_path = os.path.join(output_path, "requirements.txt")
                         if os.path.exists(req_path):
                             try:
                                 os.makedirs(os.path.dirname(output_req_path), exist_ok=True) # Ensure dir exists
                                 updated_req = self._update_python_dependencies(req_path)
                                 with open(output_req_path, 'w') as f: f.write(updated_req)
                                 all_transformations["requirements.txt"] = ["Updated dependencies (basic freeze)"]
                             except Exception as dep_e: errors.append(f"Update deps failed: {dep_e}")
                     # Add elif for package.json, pom.xml etc.

                # 6. Copy non-transformed/non-code/excluded files
                self._copy_other_files(root_path, output_path, system_info, config.excluded_paths, upgraded_files) # Pass upgraded files to avoid double copy

                # 7. Finalize Results
                time_taken = (datetime.datetime.now() - start_time).total_seconds()
                size_diff = self._calculate_size_difference(root_path, output_path)
                final_success = len(errors) == 0

                # Move from temp to final location
                final_output_path = f"{os.path.normpath(root_path)}_upgraded_{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}" # Normpath added
                try:
                    shutil.move(output_path, final_output_path) # Move from temp to final location
                    result_obj.output_path = final_output_path
                except Exception as move_e:
                    errors.append(f"Failed to move temporary output to final destination: {move_e}")
                    result_obj.output_path = output_path # Keep temp path if move fails
                    final_success = False # Mark as failed if move fails

                # Populate result object
                result_obj.success = final_success
                result_obj.upgraded_files = upgraded_files
                result_obj.errors = errors
                result_obj.time_taken_seconds = time_taken
                result_obj.size_difference = size_diff
                result_obj.applied_transformations = [f"{f}: {', '.join(t)}" for f, t in all_transformations.items()] # Flatten dict
                if system_info.license_path: result_obj.license_path = system_info.license_path # Copy license info


                self.logger.info(f"Upgrade process finished in {time_taken:.2f}s. Success: {result_obj.success}")
                return result_obj.__dict__ # Return as dict

        except Exception as e:
            self.logger.error(f"System upgrade failed: {str(e)}", exc_info=True)
            # Ensure backup_path is defined even on early failure
            backup_path = None
            if config.keep_original and 'backup_path' not in locals():
                 backup_path = self._create_backup(root_path)

            # Populate result object with failure info
            result_obj.success = False
            result_obj.errors = [f"Critical error during upgrade: {str(e)}"]
            result_obj.backup_path = backup_path
            result_obj.time_taken_seconds=(datetime.datetime.now() - start_time).total_seconds()
            return result_obj.__dict__

    # Static method for multiprocessing safety
    @staticmethod
    def _transform_file_wrapper(file_path: str, code_file: CodeFile, system_info: Optional[SystemInfo], config: UpgradeConfig, transformers: List[CodeTransformer]): # Allow None system_info
        """Wrapper function for parallel file transformation"""
        # Note: system_info might be None if only basic info is needed by transformers
        try:
            transformer = next((t for t in transformers if t.can_transform(code_file)), None)
            if transformer and config.refactor_code:
                # Pass only necessary parts of system_info if needed, or None
                new_content, file_transforms = transformer.transform(code_file, system_info) # system_info might be large
                return file_path, new_content, file_transforms, None
            else:
                # No applicable transformer or refactoring disabled
                return file_path, None, [], None # Return None for content if no change applied
        except Exception as e:
            # Log error if possible, but primarily return it
            # Using print as logger might not be configured in worker process
            print(f"ERROR transforming {file_path} in worker: {e}", file=sys.stderr)
            return file_path, None, [], f"Worker error: {str(e)}" # Return error message

    def _create_backup(self, root_path: str) -> Optional[str]: # Added Optional return type #
        """Creates a backup of the original system"""
        backup_dir = f"{os.path.normpath(root_path)}_backup_{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}" # More descriptive name #
        try:
            # Ignore common patterns like .git, node_modules etc. during backup
            ignore_patterns = shutil.ignore_patterns('.git*', 'node_modules*', '*__pycache__*', '*.pyc', '*.log', '*.tmp', 'venv*', '.venv*', 'dist', 'build', 'target')
            shutil.copytree(root_path, backup_dir, ignore=ignore_patterns, dirs_exist_ok=True) # Allow dir to exist
            self.logger.info(f"Backup created at {backup_dir}")
            return backup_dir
        except Exception as e:
            self.logger.error(f"Failed to create backup: {e}")
            return None #

    def _calculate_size_difference(self, original_path: str, new_path: str) -> int: #
        """Calculates the size difference between original and new paths"""
        try:
            orig_size = sum(p.stat().st_size for p in Path(original_path).rglob('*') if p.is_file())
            new_size = sum(p.stat().st_size for p in Path(new_path).rglob('*') if p.is_file())
            return new_size - orig_size
        except Exception as e:
            self.logger.error(f"Could not calculate size difference: {e}")
            return 0 #

    def _copy_other_files(self, src_dir, dest_dir, system_info: SystemInfo, excluded_paths: List[str], transformed_files: List[str]): # Pass transformed list
         """Copies non-code files and non-transformed files to the destination."""
         abs_excluded_paths = {os.path.abspath(os.path.join(src_dir, p)) for p in excluded_paths}
         # Use absolute paths for comparison
         abs_transformed_files = {os.path.abspath(os.path.join(src_dir, p)) for p in transformed_files}

         for root, dirs, files in os.walk(src_dir, topdown=True):
             abs_root = os.path.abspath(root)
             # Filter excluded directories
             dirs[:] = [d for d in dirs if d not in system_info.analyzer.excluded_dirs and not d.startswith('.') and os.path.join(abs_root, d) not in abs_excluded_paths] # Use analyzer's excluded dirs

             for file in files:
                 src_file_path = os.path.abspath(os.path.join(root, file))
                 rel_path = os.path.relpath(src_file_path, src_dir) # Get relative path for checks

                 # Skip if it's a transformed code file or explicitly excluded
                 if src_file_path in abs_transformed_files or \
                    src_file_path in abs_excluded_paths or \
                    any(src_file_path.startswith(ex_dir) for ex_dir in abs_excluded_paths if os.path.isdir(ex_dir)) or \
                    file in system_info.analyzer.excluded_files or file.startswith('.'): # Use analyzer's excluded files
                     continue

                 # Copy the file
                 dest_file_path = os.path.join(dest_dir, rel_path)
                 try:
                     os.makedirs(os.path.dirname(dest_file_path), exist_ok=True)
                     shutil.copy2(src_file_path, dest_file_path) # Use copy2 to preserve metadata
                 except Exception as e:
                     self.logger.warning(f"Could not copy file {src_file_path} to {dest_file_path}: {e}")

    def _update_python_dependencies(self, requirements_path: str) -> str: #
        """Updates Python dependencies using pip freeze (basic)."""
        # Placeholder: Requires pip-compile or similar tool for proper updates
        # Example using pip freeze (less accurate for updates)
        self.logger.warning("Dependency update is basic (pip freeze). Use pip-tools for better results.")
        try:
            # Determine Python executable (prefer venv if exists)
            venv_python = Path(sys.prefix) / ("Scripts" if sys.platform == 'win32' else "bin") / "python" # Check current prefix
            python_exe = str(venv_python) if Path(sys.prefix).name == 'venv' else sys.executable # Simplified check

            result = subprocess.run([python_exe, "-m", "pip", "freeze"], capture_output=True, text=True, check=True, timeout=60) # Add timeout
            return result.stdout
        except Exception as e:
            self.logger.error(f"Failed to update Python dependencies via pip freeze: {e}")
            # Fallback to original content
            try:
                with open(requirements_path, 'r') as f: return f.read()
            except IOError:
                return "# Failed to read original requirements.txt" #

# --- Main Execution Block (Example) ---
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Kaleidoscope AI - System Upgrade Module")
    parser.add_argument("path", help="Root path of the software system to upgrade")
    parser.add_argument("--target", default="python", choices=[lang.name.lower() for lang in LanguageType if lang != LanguageType.UNKNOWN], help="Target language")
    parser.add_argument("--strategy", default="in_place", choices=[strat.name.lower() for strat in UpgradeStrategy], help="Upgrade strategy")
    parser.add_argument("--no-backup", action="store_false", dest="keep_original", help="Do not create a backup")
    parser.add_argument("--no-update-deps", action="store_false", dest="update_dependencies", help="Do not update dependencies") # Allow disabling
    parser.add_argument("--no-fix-vulns", action="store_false", dest="fix_vulnerabilities", help="Do not attempt to fix vulnerabilities") # Allow disabling
    parser.add_argument("--processes", type=int, default=multiprocessing.cpu_count(), help="Max parallel processes for transformation")
    args = parser.parse_args()

    # Ensure path exists
    if not os.path.isdir(args.path):
        logger.critical(f"Input path is not a valid directory: {args.path}")
        sys.exit(1)

    # Create config
    upgrade_config = UpgradeConfig(
        target_language=LanguageType[args.target.upper()],
        strategy=UpgradeStrategy[args.strategy.upper()],
        keep_original=args.keep_original,
        update_dependencies=args.update_dependencies,
        fix_vulnerabilities=args.fix_vulnerabilities,
        max_parallel_processes=args.processes
        # Add other config args here based on argparse if needed
    )

    # Run upgrade
    upgrader = SystemUpgrader()
    result_dict = upgrader.upgrade_system(args.path, upgrade_config) #

    # Print results
    print("\n--- Upgrade Result ---")
    print(json.dumps(result_dict, indent=2, default=lambda o: o.name if isinstance(o, Enum) else o.__dict__)) # Print result dict with Enum handling

    if result_dict["success"]:
         print("\nUpgrade completed successfully!")
         print(f"Upgraded system saved to: {result_dict['output_path']}")
         if result_dict['backup_path']:
             print(f"Original system backed up to: {result_dict['backup_path']}")
    else:
         print("\nUpgrade completed with errors.")
         if result_dict['output_path'] != args.path: # Check if temp dir was moved
             print(f"Partially upgraded system (with errors) saved to: {result_dict['output_path']}")
         if result_dict['backup_path']:
             print(f"Original system backed up to: {result_dict['backup_path']}")
         print("Errors encountered:")
         for err in result_dict['errors']: print(f" - {err}")

