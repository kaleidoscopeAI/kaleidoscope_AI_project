# modules/GPTProcessor.py
import logging
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import torch
from typing import Optional, List, Dict, Any

logger = logging.getLogger(__name__)

class GPTProcessor:
    """
    Manages a locally hosted open-source GPT model for inference.
    """

    def __init__(self, model_name="EleutherAI/gpt-neo-1.3B", device="cpu"):
        """
        Initializes the GPT model and tokenizer.
        """
        self.model_name = model_name
        self.tokenizer = None
        self.model = None
        self.device = device
        self._load_model()

    def _load_model(self):
        """Loads the language model and tokenizer."""
        try:
            logger.info(f"Loading tokenizer for {self.model_name}...")
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            logger.info(f"Loading model {self.model_name} onto {self.device}...")
            self.model = AutoModelForCausalLM.from_pretrained(self.model_name, torch_dtype=torch.float32).to(self.device)
            self.model.eval()
            logger.info(f"Model {self.model_name} loaded successfully.")

        except Exception as e:
            logger.error(f"Failed to load model {self.model_name}: {e}", exc_info=True)
            self.tokenizer = None
            self.model = None

    def is_ready(self):
        """Check if the model loaded successfully."""
        return self.model is not None and self.tokenizer is not None

    def query(self, prompt: str, max_length: int = 150, num_return_sequences: int = 1) -> Optional[str]:
        """
        Generates a response using the local GPT model.
        """
        if not self.is_ready():
            logger.error("GPTProcessor is not ready. Model not loaded.")
            return None

        try:
            logger.debug(f"Querying GPT model with prompt: '{prompt[:50]}...'")
            inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512).to(self.device)

            with torch.no_grad():
                output_sequences = self.model.generate(
                    input_ids=inputs["input_ids"],
                    max_length=max_length + len(inputs["input_ids"][0]),
                    num_return_sequences=num_return_sequences,
                    do_sample=True,
                    top_k=50,
                    top_p=0.95,
                    temperature=0.7,
                    pad_token_id=self.tokenizer.eos_token_id
                )

            response_text = self.tokenizer.decode(output_sequences[0][len(inputs["input_ids"][0]):], skip_special_tokens=True)
            logger.debug(f"GPT response generated: '{response_text[:50]}...'")
            return response_text.strip()

        except Exception as e:
            logger.error(f"Error during GPT query: {e}", exc_info=True)
            return None

    # --- "Wow" Factor: Enhanced Text Analysis ---

    def analyze_text_structure(self, text: str) -> Dict[str, Any]:
        """
        Analyzes the structural properties of the text (e.g., sentence length, complexity).
        This is a placeholder; you could add more sophisticated linguistic analysis.
        """
        if not isinstance(text, str):
            logger.warning("Text structure analysis requires a string.")
            return {}

        sentences = text.split('.')
        num_sentences = len(sentences)
        avg_sentence_length = np.mean([len(s.split()) for s in sentences]) if sentences else 0
        return {
            "type": "text_structure",
            "num_sentences": num_sentences,
            "avg_sentence_length": avg_sentence_length
        }

    def extract_named_entities(self, text: str) -> List[Tuple[str, str]]:
        """
        Extracts named entities (people, organizations, locations) from the text.
        This is a placeholder; you'd likely use a dedicated NLP library.
        """
        if not isinstance(text, str):
            logger.warning("Named entity extraction requires a string.")
            return []

        # Placeholder: Replace with a real NER system (e.g., spaCy)
        # For simplicity, let's just return some dummy entities
        dummy_entities = [("AI", "ORG"), ("Kaleidoscope", "ORG"), ("the Cube", "MISC")]
        return dummy_entities

    def speculate_on_relationships(self, entities: List[Tuple[str, str]]) -> List[str]:
        """
        Generates speculative relationships between extracted entities.
        """
        if not entities:
            return []

        speculations = []
        for i in range(len(entities)):
            for j in range(i + 1, len(entities)):
                entity1, type1 = entities[i]
                entity2, type2 = entities[j]
                # Placeholder: More advanced relationship speculation
                speculations.append(f"A possible connection exists between {entity1} ({type1}) and {entity2} ({type2}).")
        return speculations
